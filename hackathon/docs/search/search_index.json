{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MedGemma Hackathon","text":"<p>This repository hosts the MedGemma hackathon demo for extracting atomic inclusion/exclusion criteria from clinical trial protocols, grounding them to SNOMED via the UMLS API, mapping field/relation/value for screening, and enabling a human-in-the-loop (HITL) review experience.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Overview: <code>docs/overview/project.md</code></li> <li>Architecture: <code>docs/overview/architecture.md</code></li> <li>Getting Started: <code>docs/overview/getting-started.md</code></li> <li>Hackathon Plan: <code>docs/overview/hackathon-plan.md</code></li> <li>API Spec: <code>docs/api/api_spec.md</code></li> <li>Components: <code>docs/components-overview.md</code></li> </ul>"},{"location":"#demo-goals","title":"Demo Goals","text":"<ul> <li>Extract atomic criteria with evidence snippets.</li> <li>Ground criteria to SNOMED with ranked candidates and confidence.</li> <li>Map criteria to field/relation/value (e.g., <code>demographics.age &gt; 75</code>).</li> <li>Provide a HITL UI for nurse review and corrections.</li> <li>Capture edits for training and evaluation.</li> </ul>"},{"location":"#repo-layout","title":"Repo Layout","text":"<pre><code>components/     Service components (API, extraction, grounding, UI, etc.)\ndocs/           MkDocs documentation\nscripts/        Utility scripts (protocol download, component creation, etc.)\ninstructions/   Planning documents\n</code></pre>"},{"location":"#scripts","title":"Scripts","text":"<p>The <code>scripts/</code> directory contains utility scripts for common tasks:</p> <ul> <li>Protocol Download: <code>scripts/download_protocol_sources.py</code> - Downloads clinical trial protocol PDFs from multiple sources (DAC, ClinicalTrials.gov, BMJ Open, JMIR)</li> <li>Component Creation: <code>scripts/create_component.sh</code> - Initializes new components with proper structure</li> <li>Documentation: <code>scripts/generate_components_overview.py</code> and <code>scripts/update_root_navigation.py</code> - Auto-generate documentation</li> </ul> <p>See Getting Started for detailed usage instructions.</p>"},{"location":"components-overview/","title":"Components Overview","text":"<p>This page lists all available components in the monorepo, grouped by category. Click a component name to view its documentation.</p>"},{"location":"components-overview/#others-components","title":"Others Components","text":"Name Description Owner Api Service api-service component Unknown Data Pipeline data-pipeline component Unknown Evaluation evaluation component Unknown Extraction Service extraction-service component Unknown Grounding Service grounding-service component Unknown Hitl Ui React/Vite frontend for nurse review of extracted criteria and evidence. Unknown Shared shared component Unknown <p>Total Components: 7</p> <p>This overview is automatically generated from component pyproject.toml files and README files.</p>"},{"location":"api/api_spec/","title":"API Spec","text":"<p>The OpenAPI spec will live in this folder as <code>api_spec.yaml</code>.</p> <p>Endpoints (minimal):</p> <ul> <li><code>POST /v1/protocols</code></li> <li><code>POST /v1/protocols/{protocolId}/extract</code></li> <li><code>GET /v1/protocols/{protocolId}/criteria</code></li> <li><code>PATCH /v1/criteria/{criterionId}</code></li> <li><code>POST /v1/criteria/{criterionId}/ground</code> (SNOMED candidates + field/relation/value mapping)</li> <li><code>POST /v1/hitl/feedback</code></li> </ul>"},{"location":"api-service/","title":"api-service","text":"<p>FastAPI service that orchestrates protocol ingestion, extraction, grounding (SNOMED + field/relation/value), and HITL feedback. This is the entry point for the demo API.</p>"},{"location":"api-service/#responsibilities","title":"Responsibilities","text":"<ul> <li>Expose REST endpoints for protocols, criteria, grounding (SNOMED + field mapping), and HITL edits.</li> <li>Validate request payloads and manage response shapes.</li> <li>Orchestrate calls to extraction and grounding components.</li> </ul>"},{"location":"api-service/#key-endpoints-wireframe","title":"Key Endpoints (wireframe)","text":"<ul> <li><code>POST /v1/protocols</code></li> <li><code>POST /v1/protocols/{protocol_id}/extract</code></li> <li><code>GET /v1/protocols/{protocol_id}/criteria</code></li> <li><code>PATCH /v1/criteria/{criterion_id}</code></li> <li><code>POST /v1/criteria/{criterion_id}/ground</code></li> <li><code>POST /v1/hitl/feedback</code></li> </ul>"},{"location":"api-service/#running-locally","title":"Running Locally","text":"<pre><code>uv sync\nuv run uvicorn api_service.main:app --reload\n</code></pre>"},{"location":"api-service/#example-usage","title":"Example Usage","text":"<pre><code>curl -X POST http://localhost:8000/v1/protocols \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"title\":\"Trial A\",\"document_text\":\"Inclusion: ...\"}'\n</code></pre>"},{"location":"api-service/#tests","title":"Tests","text":"<pre><code>make check-all\n</code></pre>"},{"location":"api-service/#configuration-planned","title":"Configuration (planned)","text":"<ul> <li><code>DATABASE_URL</code> for persistence.</li> <li><code>EXTRACTION_SERVICE_URL</code> for extraction orchestration.</li> <li><code>GROUNDING_SERVICE_URL</code> for UMLS grounding.</li> <li><code>UMLS_API_KEY</code> (or <code>GROUNDING_SERVICE_UMLS_API_KEY</code>) required for UMLS lookups.</li> <li><code>API_SERVICE_MAX_UPLOAD_BYTES</code> to cap PDF uploads (default: 20971520 bytes).</li> </ul>"},{"location":"api-service/docs/api/","title":"api-service API Reference","text":"<p>This page contains automatically generated API documentation for the api-service component.</p>"},{"location":"api-service/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"api-service/docs/api/#api_service","title":"api_service","text":"<p>api-service package.</p>"},{"location":"api-service/docs/api/#api_service-modules","title":"Modules","text":""},{"location":"api-service/docs/api/#api_service.dependencies","title":"dependencies","text":"<p>FastAPI dependencies for shared resources.</p>"},{"location":"api-service/docs/api/#api_service.dependencies-classes","title":"Classes","text":""},{"location":"api-service/docs/api/#api_service.dependencies-functions","title":"Functions","text":""},{"location":"api-service/docs/api/#api_service.dependencies.get_storage","title":"get_storage","text":"<pre><code>get_storage() -&gt; Storage\n</code></pre> <p>Provide a storage instance for request handlers.</p> Source code in <code>components/api-service/src/api_service/dependencies.py</code> <pre><code>def get_storage() -&gt; Storage:\n    \"\"\"Provide a storage instance for request handlers.\"\"\"\n    return Storage(get_engine())\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main","title":"main","text":"<p>API service wireframe for the MedGemma hackathon demo.</p>"},{"location":"api-service/docs/api/#api_service.main-classes","title":"Classes","text":""},{"location":"api-service/docs/api/#api_service.main.ApiConfig","title":"ApiConfig  <code>dataclass</code>","text":"<pre><code>ApiConfig(max_upload_bytes: int)\n</code></pre> <p>Configuration for the API service.</p> Functions from_env <code>staticmethod</code> <pre><code>from_env() -&gt; 'ApiConfig'\n</code></pre> <p>Create ApiConfig from environment variables.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@staticmethod\ndef from_env() -&gt; \"ApiConfig\":\n    \"\"\"Create ApiConfig from environment variables.\"\"\"\n    raw_value = os.getenv(\"API_SERVICE_MAX_UPLOAD_BYTES\")\n    try:\n        value = int(raw_value) if raw_value else MAX_UPLOAD_SIZE_BYTES\n    except ValueError:\n        value = MAX_UPLOAD_SIZE_BYTES\n    if value &lt;= 0:\n        value = MAX_UPLOAD_SIZE_BYTES\n    return ApiConfig(max_upload_bytes=value)\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.ProtocolCreateRequest","title":"ProtocolCreateRequest","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ProtocolCreateRequest[ProtocolCreateRequest]\n\n              \n\n              click api_service.main.ProtocolCreateRequest href \"\" \"api_service.main.ProtocolCreateRequest\"\n            </code></pre> <p>Request payload for creating a protocol entry.</p>"},{"location":"api-service/docs/api/#api_service.main.ProtocolResponse","title":"ProtocolResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ProtocolResponse[ProtocolResponse]\n\n              \n\n              click api_service.main.ProtocolResponse href \"\" \"api_service.main.ProtocolResponse\"\n            </code></pre> <p>Response payload for a created protocol.</p>"},{"location":"api-service/docs/api/#api_service.main.CriterionResponse","title":"CriterionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.CriterionResponse[CriterionResponse]\n\n              \n\n              click api_service.main.CriterionResponse href \"\" \"api_service.main.CriterionResponse\"\n            </code></pre> <p>Response payload for an extracted criterion.</p>"},{"location":"api-service/docs/api/#api_service.main.CriteriaListResponse","title":"CriteriaListResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.CriteriaListResponse[CriteriaListResponse]\n\n              \n\n              click api_service.main.CriteriaListResponse href \"\" \"api_service.main.CriteriaListResponse\"\n            </code></pre> <p>Response payload for listing criteria.</p>"},{"location":"api-service/docs/api/#api_service.main.ExtractionResponse","title":"ExtractionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ExtractionResponse[ExtractionResponse]\n\n              \n\n              click api_service.main.ExtractionResponse href \"\" \"api_service.main.ExtractionResponse\"\n            </code></pre> <p>Response payload for extraction trigger.</p>"},{"location":"api-service/docs/api/#api_service.main.CriterionUpdateRequest","title":"CriterionUpdateRequest","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.CriterionUpdateRequest[CriterionUpdateRequest]\n\n              \n\n              click api_service.main.CriterionUpdateRequest href \"\" \"api_service.main.CriterionUpdateRequest\"\n            </code></pre> <p>Payload for updating a criterion.</p>"},{"location":"api-service/docs/api/#api_service.main.CriterionUpdateResponse","title":"CriterionUpdateResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.CriterionUpdateResponse[CriterionUpdateResponse]\n\n              \n\n              click api_service.main.CriterionUpdateResponse href \"\" \"api_service.main.CriterionUpdateResponse\"\n            </code></pre> <p>Response payload after updating a criterion.</p>"},{"location":"api-service/docs/api/#api_service.main.GroundingCandidateResponse","title":"GroundingCandidateResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.GroundingCandidateResponse[GroundingCandidateResponse]\n\n              \n\n              click api_service.main.GroundingCandidateResponse href \"\" \"api_service.main.GroundingCandidateResponse\"\n            </code></pre> <p>Response payload for grounding candidates.</p>"},{"location":"api-service/docs/api/#api_service.main.FieldMappingResponse","title":"FieldMappingResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.FieldMappingResponse[FieldMappingResponse]\n\n              \n\n              click api_service.main.FieldMappingResponse href \"\" \"api_service.main.FieldMappingResponse\"\n            </code></pre> <p>Response payload for a field mapping suggestion.</p>"},{"location":"api-service/docs/api/#api_service.main.FieldMappingSuggestionRequest","title":"FieldMappingSuggestionRequest","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.FieldMappingSuggestionRequest[FieldMappingSuggestionRequest]\n\n              \n\n              click api_service.main.FieldMappingSuggestionRequest href \"\" \"api_service.main.FieldMappingSuggestionRequest\"\n            </code></pre> <p>Request payload for field mapping suggestions.</p>"},{"location":"api-service/docs/api/#api_service.main.FieldMappingSuggestionResponse","title":"FieldMappingSuggestionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.FieldMappingSuggestionResponse[FieldMappingSuggestionResponse]\n\n              \n\n              click api_service.main.FieldMappingSuggestionResponse href \"\" \"api_service.main.FieldMappingSuggestionResponse\"\n            </code></pre> <p>Response payload for field mapping suggestions.</p>"},{"location":"api-service/docs/api/#api_service.main.GroundingResponse","title":"GroundingResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.GroundingResponse[GroundingResponse]\n\n              \n\n              click api_service.main.GroundingResponse href \"\" \"api_service.main.GroundingResponse\"\n            </code></pre> <p>Response payload for grounding a criterion.</p>"},{"location":"api-service/docs/api/#api_service.main.HitlAction","title":"HitlAction","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <pre><code>\n              flowchart TD\n              api_service.main.HitlAction[HitlAction]\n\n              \n\n              click api_service.main.HitlAction href \"\" \"api_service.main.HitlAction\"\n            </code></pre> <p>HITL action types.</p>"},{"location":"api-service/docs/api/#api_service.main.HitlFeedbackRequest","title":"HitlFeedbackRequest","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.HitlFeedbackRequest[HitlFeedbackRequest]\n\n              \n\n              click api_service.main.HitlFeedbackRequest href \"\" \"api_service.main.HitlFeedbackRequest\"\n            </code></pre> <p>Payload for HITL feedback actions.</p>"},{"location":"api-service/docs/api/#api_service.main.HitlEditResponse","title":"HitlEditResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.HitlEditResponse[HitlEditResponse]\n\n              \n\n              click api_service.main.HitlEditResponse href \"\" \"api_service.main.HitlEditResponse\"\n            </code></pre> <p>Response for a single HITL edit.</p>"},{"location":"api-service/docs/api/#api_service.main.HitlEditsListResponse","title":"HitlEditsListResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.HitlEditsListResponse[HitlEditsListResponse]\n\n              \n\n              click api_service.main.HitlEditsListResponse href \"\" \"api_service.main.HitlEditsListResponse\"\n            </code></pre> <p>Response for listing HITL edits.</p>"},{"location":"api-service/docs/api/#api_service.main.ProtocolListItem","title":"ProtocolListItem","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ProtocolListItem[ProtocolListItem]\n\n              \n\n              click api_service.main.ProtocolListItem href \"\" \"api_service.main.ProtocolListItem\"\n            </code></pre> <p>Protocol summary for list view.</p>"},{"location":"api-service/docs/api/#api_service.main.ProtocolListResponse","title":"ProtocolListResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ProtocolListResponse[ProtocolListResponse]\n\n              \n\n              click api_service.main.ProtocolListResponse href \"\" \"api_service.main.ProtocolListResponse\"\n            </code></pre> <p>Response for listing protocols.</p>"},{"location":"api-service/docs/api/#api_service.main.ProtocolDetailResponse","title":"ProtocolDetailResponse","text":"<p>               Bases: <code>BaseModel</code></p> <pre><code>\n              flowchart TD\n              api_service.main.ProtocolDetailResponse[ProtocolDetailResponse]\n\n              \n\n              click api_service.main.ProtocolDetailResponse href \"\" \"api_service.main.ProtocolDetailResponse\"\n            </code></pre> <p>Response for protocol detail view.</p>"},{"location":"api-service/docs/api/#api_service.main-functions","title":"Functions","text":""},{"location":"api-service/docs/api/#api_service.main.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; ApiConfig\n</code></pre> <p>Get the current API configuration.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>def get_config() -&gt; ApiConfig:\n    \"\"\"Get the current API configuration.\"\"\"\n    return ApiConfig.from_env()\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.lifespan","title":"lifespan  <code>async</code>","text":"<pre><code>lifespan(app_instance: FastAPI) -&gt; AsyncIterator[None]\n</code></pre> <p>Initialize and teardown app state for the lifespan scope.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@asynccontextmanager\nasync def lifespan(app_instance: FastAPI) -&gt; AsyncIterator[None]:\n    \"\"\"Initialize and teardown app state for the lifespan scope.\"\"\"\n    init_db()\n    # Validate required configuration\n    api_key = os.getenv(\"GROUNDING_SERVICE_UMLS_API_KEY\") or os.getenv(\"UMLS_API_KEY\")\n    if not api_key:\n        raise RuntimeError(\"UMLS_API_KEY or GROUNDING_SERVICE_UMLS_API_KEY must be set\")\n    yield\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.create_protocol","title":"create_protocol","text":"<pre><code>create_protocol(payload: ProtocolCreateRequest, storage: Storage = Depends(get_storage)) -&gt; ProtocolResponse\n</code></pre> <p>Create a protocol record and initial document entry.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/protocols\")\ndef create_protocol(\n    payload: ProtocolCreateRequest,\n    storage: Storage = Depends(get_storage),\n) -&gt; ProtocolResponse:\n    \"\"\"Create a protocol record and initial document entry.\"\"\"\n    protocol = storage.create_protocol(\n        title=payload.title,\n        document_text=payload.document_text,\n        nct_id=payload.nct_id,\n        condition=payload.condition,\n        phase=payload.phase,\n    )\n    return ProtocolResponse(protocol_id=protocol.id, title=protocol.title)\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.upload_protocol","title":"upload_protocol  <code>async</code>","text":"<pre><code>upload_protocol(file: UploadFile = File(...), auto_extract: bool = True, background_tasks: BackgroundTasks = BackgroundTasks(), storage: Storage = Depends(get_storage)) -&gt; ProtocolResponse\n</code></pre> <p>Upload a PDF protocol file and create a protocol record.</p> <p>If auto_extract is True, extraction runs asynchronously in the background after the response is returned to avoid request timeouts for large PDFs.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/protocols/upload\")\nasync def upload_protocol(\n    file: UploadFile = File(...),\n    auto_extract: bool = True,\n    background_tasks: BackgroundTasks = BackgroundTasks(),\n    storage: Storage = Depends(get_storage),\n) -&gt; ProtocolResponse:\n    \"\"\"Upload a PDF protocol file and create a protocol record.\n\n    If auto_extract is True, extraction runs asynchronously in the background\n    after the response is returned to avoid request timeouts for large PDFs.\n    \"\"\"\n    filename = file.filename or \"\"\n    if file.content_type != \"application/pdf\" or not filename.lower().endswith(\".pdf\"):\n        raise HTTPException(\n            status_code=415, detail=\"Unsupported media type; only PDF is allowed\"\n        )\n\n    bytes_read = 0\n    with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as tmp:\n        tmp_path = Path(tmp.name)\n        while True:\n            chunk = await file.read(1024 * 1024)\n            if not chunk:\n                break\n            bytes_read += len(chunk)\n            if bytes_read &gt; get_config().max_upload_bytes:\n                tmp_path.unlink(missing_ok=True)\n                raise HTTPException(status_code=413, detail=\"File too large\")\n            tmp.write(chunk)\n\n    with tmp_path.open(\"rb\") as handle:\n        header = handle.read(4)\n    if header != b\"%PDF\":\n        tmp_path.unlink(missing_ok=True)\n        raise HTTPException(status_code=400, detail=\"Invalid PDF file\")\n\n    try:\n        document_text = await to_thread.run_sync(extract_text_from_pdf, tmp_path)\n    finally:\n        tmp_path.unlink(missing_ok=True)\n\n    if not document_text:\n        raise HTTPException(\n            status_code=400, detail=\"No text could be extracted from the PDF\"\n        )\n\n    title = filename.replace(\".pdf\", \"\").replace(\"_\", \" \").strip() or \"Protocol\"\n    protocol = storage.create_protocol(title=title, document_text=document_text)\n\n    if auto_extract:\n        # Run extraction in background to avoid blocking the response\n        background_tasks.add_task(\n            _run_extraction, protocol.id, protocol.document_text, storage\n        )\n\n    return ProtocolResponse(protocol_id=protocol.id, title=protocol.title)\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.extract_criteria","title":"extract_criteria","text":"<pre><code>extract_criteria(protocol_id: str, background_tasks: BackgroundTasks = BackgroundTasks(), storage: Storage = Depends(get_storage)) -&gt; ExtractionResponse\n</code></pre> <p>Trigger extraction of atomic criteria for a protocol.</p> <p>Extraction runs asynchronously in the background after the response is returned to avoid request timeouts for long documents.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/protocols/{protocol_id}/extract\")\ndef extract_criteria(\n    protocol_id: str,\n    background_tasks: BackgroundTasks = BackgroundTasks(),\n    storage: Storage = Depends(get_storage),\n) -&gt; ExtractionResponse:\n    \"\"\"Trigger extraction of atomic criteria for a protocol.\n\n    Extraction runs asynchronously in the background after the response is returned\n    to avoid request timeouts for long documents.\n    \"\"\"\n    protocol = storage.get_protocol(protocol_id)\n    if protocol is None:\n        raise HTTPException(status_code=404, detail=\"Protocol not found\")\n\n    # Run extraction in background to avoid blocking the response\n    background_tasks.add_task(\n        _run_extraction, protocol_id, protocol.document_text, storage\n    )\n\n    return ExtractionResponse(\n        protocol_id=protocol_id, status=\"processing\", criteria_count=0\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.list_protocols","title":"list_protocols","text":"<pre><code>list_protocols(skip: int = 0, limit: int = 20, storage: Storage = Depends(get_storage)) -&gt; ProtocolListResponse\n</code></pre> <p>List all protocols with pagination.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.get(\"/v1/protocols\")\ndef list_protocols(\n    skip: int = 0,\n    limit: int = 20,\n    storage: Storage = Depends(get_storage),\n) -&gt; ProtocolListResponse:\n    \"\"\"List all protocols with pagination.\"\"\"\n    if skip &lt; 0 or limit &lt;= 0 or limit &gt; 100:\n        raise HTTPException(status_code=400, detail=\"Invalid pagination parameters\")\n\n    protocols, total = storage.list_protocols(skip=skip, limit=limit)\n    return ProtocolListResponse(\n        protocols=[\n            ProtocolListItem(\n                protocol_id=protocol.id,\n                title=protocol.title,\n                nct_id=protocol.nct_id,\n                condition=protocol.condition,\n                phase=protocol.phase,\n            )\n            for protocol in protocols\n        ],\n        total=total,\n        skip=skip,\n        limit=limit,\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.get_protocol","title":"get_protocol","text":"<pre><code>get_protocol(protocol_id: str, storage: Storage = Depends(get_storage)) -&gt; ProtocolDetailResponse\n</code></pre> <p>Get protocol details.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.get(\"/v1/protocols/{protocol_id}\")\ndef get_protocol(\n    protocol_id: str,\n    storage: Storage = Depends(get_storage),\n) -&gt; ProtocolDetailResponse:\n    \"\"\"Get protocol details.\"\"\"\n    protocol = storage.get_protocol(protocol_id)\n    if protocol is None:\n        raise HTTPException(status_code=404, detail=\"Protocol not found\")\n\n    criteria_count = storage.count_criteria(protocol_id)\n\n    return ProtocolDetailResponse(\n        protocol_id=protocol.id,\n        title=protocol.title,\n        document_text=protocol.document_text,\n        nct_id=protocol.nct_id,\n        condition=protocol.condition,\n        phase=protocol.phase,\n        criteria_count=criteria_count,\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.list_criteria","title":"list_criteria","text":"<pre><code>list_criteria(protocol_id: str, storage: Storage = Depends(get_storage)) -&gt; CriteriaListResponse\n</code></pre> <p>List criteria generated for a protocol.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.get(\"/v1/protocols/{protocol_id}/criteria\")\ndef list_criteria(\n    protocol_id: str,\n    storage: Storage = Depends(get_storage),\n) -&gt; CriteriaListResponse:\n    \"\"\"List criteria generated for a protocol.\"\"\"\n    protocol = storage.get_protocol(protocol_id)\n    if protocol is None:\n        raise HTTPException(status_code=404, detail=\"Protocol not found\")\n\n    criteria = [\n        _criterion_to_response(criterion)\n        for criterion in storage.list_criteria(protocol_id)\n    ]\n    return CriteriaListResponse(protocol_id=protocol_id, criteria=criteria)\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.update_criterion","title":"update_criterion","text":"<pre><code>update_criterion(criterion_id: str, payload: CriterionUpdateRequest | None = Body(default=None), storage: Storage = Depends(get_storage)) -&gt; CriterionUpdateResponse\n</code></pre> <p>Update a single criterion or its metadata.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.patch(\"/v1/criteria/{criterion_id}\")\ndef update_criterion(\n    criterion_id: str,\n    payload: CriterionUpdateRequest | None = Body(default=None),\n    storage: Storage = Depends(get_storage),\n) -&gt; CriterionUpdateResponse:\n    \"\"\"Update a single criterion or its metadata.\"\"\"\n    updates = payload.model_dump(exclude_unset=True) if payload else {}\n    criterion = storage.update_criterion(\n        criterion_id=criterion_id,\n        text=updates.get(\"text\"),\n        criterion_type=updates.get(\"criterion_type\"),\n    )\n    if criterion is None:\n        raise HTTPException(status_code=404, detail=\"Criterion not found\")\n    return CriterionUpdateResponse(\n        criterion_id=criterion_id,\n        status=\"updated\",\n        criterion=_criterion_to_response(criterion),\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.suggest_field_mapping","title":"suggest_field_mapping","text":"<pre><code>suggest_field_mapping(payload: FieldMappingSuggestionRequest) -&gt; FieldMappingSuggestionResponse\n</code></pre> <p>Suggest field mappings for a criterion text.</p> <p>This endpoint uses the same logic as the grounding service to propose field/relation/value mappings, allowing frontend components to get suggestions without duplicating the regex logic.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/criteria/suggest-mapping\")\ndef suggest_field_mapping(\n    payload: FieldMappingSuggestionRequest,\n) -&gt; FieldMappingSuggestionResponse:\n    \"\"\"Suggest field mappings for a criterion text.\n\n    This endpoint uses the same logic as the grounding service to propose\n    field/relation/value mappings, allowing frontend components to get\n    suggestions without duplicating the regex logic.\n    \"\"\"\n    if not payload.criterion_text.strip():\n        raise HTTPException(\n            status_code=400, detail=\"criterion_text cannot be empty\"\n        )\n\n    field_mappings = umls_client.propose_field_mapping(payload.criterion_text)\n\n    suggestions = [\n        FieldMappingResponse(\n            field=mapping.field,\n            relation=mapping.relation,\n            value=mapping.value,\n            confidence=mapping.confidence,\n        )\n        for mapping in field_mappings\n    ]\n\n    return FieldMappingSuggestionResponse(suggestions=suggestions)\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.ground_criterion","title":"ground_criterion","text":"<pre><code>ground_criterion(criterion_id: str, storage: Storage = Depends(get_storage)) -&gt; GroundingResponse\n</code></pre> <p>Retrieve SNOMED candidates and field mappings for a criterion.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/criteria/{criterion_id}/ground\")\ndef ground_criterion(\n    criterion_id: str,\n    storage: Storage = Depends(get_storage),\n) -&gt; GroundingResponse:\n    \"\"\"Retrieve SNOMED candidates and field mappings for a criterion.\"\"\"\n    criterion = storage.get_criterion(criterion_id)\n    if criterion is None:\n        raise HTTPException(status_code=404, detail=\"Criterion not found\")\n\n    with umls_client.UmlsClient(api_key=_get_umls_api_key()) as client:\n        candidates = client.search_snomed(criterion.text)\n        field_mappings = umls_client.propose_field_mapping(criterion.text)\n\n        if not candidates:\n            storage.set_snomed_codes(criterion_id=criterion_id, snomed_codes=[])\n            return GroundingResponse(\n                criterion_id=criterion_id,\n                candidates=[],\n                field_mapping=None,\n            )\n\n        snomed_codes = [candidate.code for candidate in candidates]\n        storage.set_snomed_codes(\n            criterion_id=criterion_id,\n            snomed_codes=snomed_codes,\n        )\n\n        response_candidates = [\n            GroundingCandidateResponse(\n                code=candidate.code,\n                display=candidate.display,\n                ontology=candidate.ontology,\n                confidence=candidate.confidence,\n            )\n            for candidate in candidates\n        ]\n\n        field_mapping = None\n        if field_mappings:\n            suggestion = field_mappings[0]\n            field_mapping = FieldMappingResponse(\n                field=suggestion.field,\n                relation=suggestion.relation,\n                value=suggestion.value,\n                confidence=suggestion.confidence,\n            )\n\n        return GroundingResponse(\n            criterion_id=criterion_id,\n            candidates=response_candidates,\n            field_mapping=field_mapping,\n        )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.hitl_feedback","title":"hitl_feedback","text":"<pre><code>hitl_feedback(payload: HitlFeedbackRequest | None = None, storage: Storage = Depends(get_storage)) -&gt; dict[str, str]\n</code></pre> <p>Record HITL feedback for criteria, SNOMED candidates, and field mappings.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.post(\"/v1/hitl/feedback\")\ndef hitl_feedback(\n    payload: HitlFeedbackRequest | None = None,\n    storage: Storage = Depends(get_storage),\n) -&gt; dict[str, str]:\n    \"\"\"Record HITL feedback for criteria, SNOMED candidates, and field mappings.\"\"\"\n    if payload is None:\n        raise HTTPException(status_code=400, detail=\"Missing feedback payload\")\n\n    # Ensure criterion exists\n    criterion = storage.get_criterion(payload.criterion_id)\n    if criterion is None:\n        raise HTTPException(status_code=404, detail=\"Criterion not found\")\n    if payload.action == HitlAction.add_code and not payload.snomed_code_added:\n        raise HTTPException(\n            status_code=400, detail=\"snomed_code_added is required for add_code\"\n        )\n    if payload.action == HitlAction.remove_code and not payload.snomed_code_removed:\n        raise HTTPException(\n            status_code=400, detail=\"snomed_code_removed is required for remove_code\"\n        )\n    if payload.action == HitlAction.add_mapping and not payload.field_mapping_added:\n        raise HTTPException(\n            status_code=400, detail=\"field_mapping_added is required for add_mapping\"\n        )\n    if (\n        payload.action == HitlAction.remove_mapping\n        and not payload.field_mapping_removed\n    ):\n        raise HTTPException(\n            status_code=400,\n            detail=\"field_mapping_removed is required for remove_mapping\",\n        )\n    storage.create_hitl_edit(\n        criterion_id=payload.criterion_id,\n        action=payload.action.value,\n        snomed_code_added=payload.snomed_code_added,\n        snomed_code_removed=payload.snomed_code_removed,\n        field_mapping_added=payload.field_mapping_added,\n        field_mapping_removed=payload.field_mapping_removed,\n        note=payload.note,\n    )\n\n    if payload.snomed_code_added:\n        storage.add_snomed_code(payload.criterion_id, payload.snomed_code_added)\n    if payload.snomed_code_removed:\n        storage.remove_snomed_code(payload.criterion_id, payload.snomed_code_removed)\n    return {\"status\": \"recorded\"}\n</code></pre>"},{"location":"api-service/docs/api/#api_service.main.list_criterion_edits","title":"list_criterion_edits","text":"<pre><code>list_criterion_edits(criterion_id: str, storage: Storage = Depends(get_storage)) -&gt; HitlEditsListResponse\n</code></pre> <p>List all HITL edits for a criterion.</p> Source code in <code>components/api-service/src/api_service/main.py</code> <pre><code>@app.get(\"/v1/criteria/{criterion_id}/edits\")\ndef list_criterion_edits(\n    criterion_id: str,\n    storage: Storage = Depends(get_storage),\n) -&gt; HitlEditsListResponse:\n    \"\"\"List all HITL edits for a criterion.\"\"\"\n    edits = storage.list_hitl_edits(criterion_id)\n    return HitlEditsListResponse(\n        criterion_id=criterion_id,\n        edits=[\n            HitlEditResponse(\n                id=edit.id,\n                criterion_id=edit.criterion_id,\n                action=HitlAction(edit.action),\n                snomed_code_added=edit.snomed_code_added,\n                snomed_code_removed=edit.snomed_code_removed,\n                field_mapping_added=edit.field_mapping_added,\n                field_mapping_removed=edit.field_mapping_removed,\n                note=edit.note,\n                created_at=edit.created_at,\n            )\n            for edit in edits\n        ],\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.storage","title":"storage","text":"<p>Storage layer for the API service.</p>"},{"location":"api-service/docs/api/#api_service.storage-classes","title":"Classes","text":""},{"location":"api-service/docs/api/#api_service.storage.ExtractedCriterion","title":"ExtractedCriterion","text":"<p>               Bases: <code>Protocol</code></p> <pre><code>\n              flowchart TD\n              api_service.storage.ExtractedCriterion[ExtractedCriterion]\n\n              \n\n              click api_service.storage.ExtractedCriterion href \"\" \"api_service.storage.ExtractedCriterion\"\n            </code></pre> <p>Protocol for extracted criteria returned by the pipeline.</p>"},{"location":"api-service/docs/api/#api_service.storage.Protocol","title":"Protocol","text":"<p>               Bases: <code>SQLModel</code></p> <pre><code>\n              flowchart TD\n              api_service.storage.Protocol[Protocol]\n\n              \n\n              click api_service.storage.Protocol href \"\" \"api_service.storage.Protocol\"\n            </code></pre> <p>Protocol record persisted for API requests.</p>"},{"location":"api-service/docs/api/#api_service.storage.Criterion","title":"Criterion","text":"<p>               Bases: <code>SQLModel</code></p> <pre><code>\n              flowchart TD\n              api_service.storage.Criterion[Criterion]\n\n              \n\n              click api_service.storage.Criterion href \"\" \"api_service.storage.Criterion\"\n            </code></pre> <p>Criterion record persisted for API requests.</p>"},{"location":"api-service/docs/api/#api_service.storage.HitlEdit","title":"HitlEdit","text":"<p>               Bases: <code>SQLModel</code></p> <pre><code>\n              flowchart TD\n              api_service.storage.HitlEdit[HitlEdit]\n\n              \n\n              click api_service.storage.HitlEdit href \"\" \"api_service.storage.HitlEdit\"\n            </code></pre> <p>HITL edit record for tracking reviewer changes.</p>"},{"location":"api-service/docs/api/#api_service.storage.IdCounter","title":"IdCounter","text":"<p>               Bases: <code>SQLModel</code></p> <pre><code>\n              flowchart TD\n              api_service.storage.IdCounter[IdCounter]\n\n              \n\n              click api_service.storage.IdCounter href \"\" \"api_service.storage.IdCounter\"\n            </code></pre> <p>Deprecated: Counter table no longer used.</p> <p>Replaced with UUID-based identifiers for concurrency safety. This table is kept for backward compatibility but is no longer populated.</p>"},{"location":"api-service/docs/api/#api_service.storage.Storage","title":"Storage","text":"<pre><code>Storage(engine: Engine)\n</code></pre> <p>Repository wrapper around SQLModel sessions.</p> <p>Initialize the storage with a database engine.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def __init__(self, engine: Engine) -&gt; None:\n    \"\"\"Initialize the storage with a database engine.\"\"\"\n    self._engine = engine\n</code></pre> Functions create_protocol <pre><code>create_protocol(*, title: str, document_text: str, nct_id: str | None = None, condition: str | None = None, phase: str | None = None, source: str | None = None, registry_id: str | None = None, registry_type: str | None = None) -&gt; Protocol\n</code></pre> <p>Persist a protocol record and return it.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def create_protocol(\n    self,\n    *,\n    title: str,\n    document_text: str,\n    nct_id: str | None = None,\n    condition: str | None = None,\n    phase: str | None = None,\n    source: str | None = None,\n    registry_id: str | None = None,\n    registry_type: str | None = None,\n) -&gt; Protocol:\n    \"\"\"Persist a protocol record and return it.\"\"\"\n    with Session(self._engine) as session:\n        protocol_id = _generate_id(\"proto\")\n        protocol = Protocol(\n            id=protocol_id,\n            title=title.strip(),\n            document_text=document_text,\n            nct_id=_norm_opt(nct_id),\n            condition=_norm_opt(condition),\n            phase=_norm_opt(phase),\n            source=_norm_opt(source),\n            registry_id=_norm_opt(registry_id),\n            registry_type=_norm_opt(registry_type),\n        )\n        session.add(protocol)\n        session.commit()\n        session.refresh(protocol)\n        return protocol\n</code></pre> create_protocol_from_shared <pre><code>create_protocol_from_shared(shared: Protocol, document_text: str) -&gt; Protocol\n</code></pre> <p>Create a Protocol from a shared Protocol model and document text.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def create_protocol_from_shared(\n    self, shared: SharedProtocol, document_text: str\n) -&gt; Protocol:\n    \"\"\"Create a Protocol from a shared Protocol model and document text.\"\"\"\n    with Session(self._engine) as session:\n        protocol_id = _generate_id(\"proto\")\n        protocol = Protocol(\n            id=protocol_id,\n            title=shared.title.strip(),\n            document_text=document_text,\n            nct_id=_norm_opt(getattr(shared, \"nct_id\", None)),\n            condition=_norm_opt(getattr(shared, \"condition\", None)),\n            phase=_norm_opt(getattr(shared, \"phase\", None)),\n            source=_norm_opt(getattr(shared, \"source\", None)),\n            registry_id=_norm_opt(getattr(shared, \"registry_id\", None)),\n            registry_type=_norm_opt(getattr(shared, \"registry_type\", None)),\n        )\n        session.add(protocol)\n        session.commit()\n        session.refresh(protocol)\n        return protocol\n</code></pre> get_protocol <pre><code>get_protocol(protocol_id: str) -&gt; Protocol | None\n</code></pre> <p>Fetch a protocol by ID.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def get_protocol(self, protocol_id: str) -&gt; Protocol | None:\n    \"\"\"Fetch a protocol by ID.\"\"\"\n    with Session(self._engine) as session:\n        return session.get(Protocol, protocol_id)\n</code></pre> list_criteria <pre><code>list_criteria(protocol_id: str) -&gt; list[Criterion]\n</code></pre> <p>List criteria for a protocol.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def list_criteria(self, protocol_id: str) -&gt; list[Criterion]:\n    \"\"\"List criteria for a protocol.\"\"\"\n    with Session(self._engine) as session:\n        # Keep full models for simplicity; revisit partial selects if needed later.\n        statement = (\n            select(Criterion)\n            .where(cast(Any, Criterion.protocol_id) == protocol_id)\n            .order_by(Criterion.id)\n        )\n        return list(session.exec(statement))\n</code></pre> count_criteria <pre><code>count_criteria(protocol_id: str) -&gt; int\n</code></pre> <p>Return count of criteria for a protocol without loading all rows.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def count_criteria(self, protocol_id: str) -&gt; int:\n    \"\"\"Return count of criteria for a protocol without loading all rows.\"\"\"\n    with Session(self._engine) as session:\n        result = session.exec(\n            select(func.count(col(Criterion.id))).where(\n                cast(Any, Criterion.protocol_id) == protocol_id\n            )\n        ).one()\n        return int(result)\n</code></pre> replace_criteria <pre><code>replace_criteria(*, protocol_id: str, extracted: Iterable[ExtractedCriterion]) -&gt; list[Criterion]\n</code></pre> <p>Replace criteria for a protocol with extracted entries.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def replace_criteria(\n    self, *, protocol_id: str, extracted: Iterable[ExtractedCriterion]\n) -&gt; list[Criterion]:\n    \"\"\"Replace criteria for a protocol with extracted entries.\"\"\"\n    with Session(self._engine) as session:\n        session.exec(\n            delete(Criterion).where(cast(Any, Criterion.protocol_id) == protocol_id)\n        )\n        stored: list[Criterion] = []\n        for item in extracted:\n            criterion_id = _generate_id(\"crit\")\n            criterion = Criterion(\n                id=criterion_id,\n                protocol_id=protocol_id,\n                text=item.text,\n                criterion_type=item.criterion_type,\n                confidence=item.confidence,\n                snomed_codes=[],\n            )\n            session.add(criterion)\n            stored.append(criterion)\n        session.commit()\n        return stored\n</code></pre> update_criterion <pre><code>update_criterion(*, criterion_id: str, text: str | None, criterion_type: str | None) -&gt; Criterion | None\n</code></pre> <p>Update a criterion's text/type and return the updated row.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def update_criterion(\n    self,\n    *,\n    criterion_id: str,\n    text: str | None,\n    criterion_type: str | None,\n) -&gt; Criterion | None:\n    \"\"\"Update a criterion's text/type and return the updated row.\"\"\"\n    with Session(self._engine) as session:\n        criterion = session.get(Criterion, criterion_id)\n        if criterion is None:\n            return None\n        if text is not None:\n            criterion.text = text\n        if criterion_type is not None:\n            criterion.criterion_type = criterion_type\n        session.add(criterion)\n        session.commit()\n        session.refresh(criterion)\n        return criterion\n</code></pre> get_criterion <pre><code>get_criterion(criterion_id: str) -&gt; Criterion | None\n</code></pre> <p>Fetch a criterion by ID.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def get_criterion(self, criterion_id: str) -&gt; Criterion | None:\n    \"\"\"Fetch a criterion by ID.\"\"\"\n    with Session(self._engine) as session:\n        return session.get(Criterion, criterion_id)\n</code></pre> set_snomed_codes <pre><code>set_snomed_codes(*, criterion_id: str, snomed_codes: list[str]) -&gt; Criterion | None\n</code></pre> <p>Set SNOMED codes for a criterion.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def set_snomed_codes(\n    self, *, criterion_id: str, snomed_codes: list[str]\n) -&gt; Criterion | None:\n    \"\"\"Set SNOMED codes for a criterion.\"\"\"\n    with Session(self._engine) as session:\n        criterion = session.get(Criterion, criterion_id)\n        if criterion is None:\n            return None\n        criterion.snomed_codes = snomed_codes\n        session.add(criterion)\n        session.commit()\n        session.refresh(criterion)\n        return criterion\n</code></pre> add_snomed_code <pre><code>add_snomed_code(criterion_id: str, code: str) -&gt; Criterion | None\n</code></pre> <p>Add a SNOMED code to a criterion.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def add_snomed_code(self, criterion_id: str, code: str) -&gt; Criterion | None:\n    \"\"\"Add a SNOMED code to a criterion.\"\"\"\n    with Session(self._engine) as session:\n        criterion = session.get(Criterion, criterion_id)\n        if criterion is None:\n            return None\n        if code not in criterion.snomed_codes:\n            criterion.snomed_codes = [*criterion.snomed_codes, code]\n            session.add(criterion)\n            session.commit()\n            session.refresh(criterion)\n        return criterion\n</code></pre> remove_snomed_code <pre><code>remove_snomed_code(criterion_id: str, code: str) -&gt; Criterion | None\n</code></pre> <p>Remove a SNOMED code from a criterion.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def remove_snomed_code(self, criterion_id: str, code: str) -&gt; Criterion | None:\n    \"\"\"Remove a SNOMED code from a criterion.\"\"\"\n    with Session(self._engine) as session:\n        criterion = session.get(Criterion, criterion_id)\n        if criterion is None:\n            return None\n        if code in criterion.snomed_codes:\n            criterion.snomed_codes = [\n                existing for existing in criterion.snomed_codes if existing != code\n            ]\n            session.add(criterion)\n            session.commit()\n            session.refresh(criterion)\n        return criterion\n</code></pre> list_protocols <pre><code>list_protocols(skip: int = 0, limit: int = 20) -&gt; tuple[list[Protocol], int]\n</code></pre> <p>List protocols with pagination.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def list_protocols(\n    self, skip: int = 0, limit: int = 20\n) -&gt; tuple[list[Protocol], int]:\n    \"\"\"List protocols with pagination.\"\"\"\n    with Session(self._engine) as session:\n        total = session.exec(select(func.count(col(Protocol.id)))).one()\n        # Order by title for consistent ordering (UUIDs are not ordered)\n        statement = (\n            select(Protocol).offset(skip).limit(limit).order_by(Protocol.title)\n        )\n        protocols = list(session.exec(statement))\n        return protocols, int(total)\n</code></pre> create_hitl_edit <pre><code>create_hitl_edit(*, criterion_id: str, action: str, snomed_code_added: str | None = None, snomed_code_removed: str | None = None, field_mapping_added: str | None = None, field_mapping_removed: str | None = None, note: str | None = None) -&gt; HitlEdit\n</code></pre> <p>Persist a HITL edit record.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def create_hitl_edit(\n    self,\n    *,\n    criterion_id: str,\n    action: str,\n    snomed_code_added: str | None = None,\n    snomed_code_removed: str | None = None,\n    field_mapping_added: str | None = None,\n    field_mapping_removed: str | None = None,\n    note: str | None = None,\n) -&gt; HitlEdit:\n    \"\"\"Persist a HITL edit record.\"\"\"\n    with Session(self._engine) as session:\n        edit_id = _generate_id(\"edit\")\n        edit = HitlEdit(\n            id=edit_id,\n            criterion_id=criterion_id,\n            action=action,\n            snomed_code_added=snomed_code_added,\n            snomed_code_removed=snomed_code_removed,\n            field_mapping_added=field_mapping_added,\n            field_mapping_removed=field_mapping_removed,\n            note=note,\n        )\n        session.add(edit)\n        session.commit()\n        session.refresh(edit)\n        return edit\n</code></pre> list_hitl_edits <pre><code>list_hitl_edits(criterion_id: str) -&gt; list[HitlEdit]\n</code></pre> <p>List all HITL edits for a criterion.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def list_hitl_edits(self, criterion_id: str) -&gt; list[HitlEdit]:\n    \"\"\"List all HITL edits for a criterion.\"\"\"\n    with Session(self._engine) as session:\n        statement = (\n            select(HitlEdit)\n            .where(cast(Any, HitlEdit.criterion_id) == criterion_id)\n            .order_by(cast(Any, HitlEdit.created_at))\n        )\n        return list(session.exec(statement))\n</code></pre>"},{"location":"api-service/docs/api/#api_service.storage-functions","title":"Functions","text":""},{"location":"api-service/docs/api/#api_service.storage.get_engine","title":"get_engine  <code>cached</code>","text":"<pre><code>get_engine() -&gt; Engine\n</code></pre> <p>Create or return the cached database engine.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>@lru_cache\ndef get_engine() -&gt; Engine:\n    \"\"\"Create or return the cached database engine.\"\"\"\n    db_path = DEFAULT_DB_PATH\n    if \"DATABASE_URL\" not in os.environ and \"API_SERVICE_DB_URL\" not in os.environ:\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n    return create_engine(\n        _database_url(),\n        connect_args={\"check_same_thread\": False},\n    )\n</code></pre>"},{"location":"api-service/docs/api/#api_service.storage.init_db","title":"init_db","text":"<pre><code>init_db() -&gt; None\n</code></pre> <p>Initialize the database tables.</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def init_db() -&gt; None:\n    \"\"\"Initialize the database tables.\"\"\"\n    SQLModel.metadata.create_all(get_engine())\n</code></pre>"},{"location":"api-service/docs/api/#api_service.storage.reset_storage","title":"reset_storage","text":"<pre><code>reset_storage() -&gt; None\n</code></pre> <p>Clear all stored data (used for tests and demos).</p> Source code in <code>components/api-service/src/api_service/storage.py</code> <pre><code>def reset_storage() -&gt; None:\n    \"\"\"Clear all stored data (used for tests and demos).\"\"\"\n    if os.getenv(\"ALLOW_STORAGE_RESET\") != \"1\":\n        raise RuntimeError(\n            \"reset_storage() requires ALLOW_STORAGE_RESET=1 environment variable. \"\n            \"This function destroys all data and should only be used in tests.\"\n        )\n    engine = get_engine()\n    SQLModel.metadata.drop_all(engine)\n    SQLModel.metadata.create_all(engine)\n    init_db()\n</code></pre>"},{"location":"data-pipeline/","title":"data-pipeline","text":"<p>Protocol ingestion tools for ClinicalTrials.gov. Converts raw protocol documents into normalized records.</p>"},{"location":"data-pipeline/#responsibilities","title":"Responsibilities","text":"<ul> <li>Download or ingest protocol text/PDFs.</li> <li>Normalize and store protocol metadata.</li> <li>Emit data for extraction and grounding.</li> </ul>"},{"location":"data-pipeline/#protocol-download","title":"Protocol Download","text":"<p>To download protocol PDFs from multiple sources (DAC, ClinicalTrials.gov, BMJ Open, JMIR), use the protocol download script in the root <code>scripts/</code> directory:</p> <pre><code># From repository root\npython scripts/download_protocol_sources.py\n\n# With options\npython scripts/download_protocol_sources.py --sources dac jmir --max-per-source 20\n</code></pre> <p>See the main Getting Started documentation for full usage details.</p>"},{"location":"data-pipeline/#entry-point","title":"Entry Point","text":"<ul> <li><code>data_pipeline/download_protocols.py</code></li> <li><code>data_pipeline/loader.py</code></li> </ul>"},{"location":"data-pipeline/#running-the-stub","title":"Running the Stub","text":"<pre><code># Ensure PDFs and manifest exist first\npython scripts/download_protocol_sources.py --max-per-source 10\n\n# Ingest downloaded PDFs\nuv run python -m data_pipeline.download_protocols --manifest-path data/protocols/manifest.jsonl\n</code></pre>"},{"location":"data-pipeline/#load-into-the-api-database","title":"Load Into the API Database","text":"<p>Use the loader to import extracted PDFs into the API database.</p> <pre><code># Single PDF\nuv run python -m data_pipeline.loader --pdf data/protocols/example.pdf --api-url http://localhost:8000\n\n# Bulk from manifest\nuv run python -m data_pipeline.loader --manifest data/protocols/manifest.jsonl --limit 50\n\n# Without auto-extraction\nuv run python -m data_pipeline.loader --pdf data/protocols/example.pdf --no-extract\n</code></pre> <p>The API also exposes a PDF upload endpoint:</p> <pre><code>curl -X POST http://localhost:8000/v1/protocols/upload \\\n  -F \"file=@data/protocols/example.pdf\" \\\n  -F \"auto_extract=true\"\n</code></pre>"},{"location":"data-pipeline/#planned-outputs","title":"Planned Outputs","text":"<ul> <li><code>protocols</code> table rows (<code>nct_id</code>, title, condition, phase).</li> <li><code>documents</code> table rows for protocol text.</li> </ul>"},{"location":"data-pipeline/docs/api/","title":"data-pipeline API Reference","text":"<p>This page contains automatically generated API documentation for the data-pipeline component.</p>"},{"location":"data-pipeline/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"data-pipeline/docs/api/#data_pipeline","title":"data_pipeline","text":"<p>data-pipeline package.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline-classes","title":"Classes","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.ProtocolRecord","title":"ProtocolRecord  <code>dataclass</code>","text":"<pre><code>ProtocolRecord(nct_id: str, title: str, condition: str, phase: str, document_text: str, source: str | None = None, registry_id: str | None = None, registry_type: str | None = None, source_url: str | None = None)\n</code></pre> <p>Normalized protocol record for downstream services.</p> <p>Parameters:</p> Name Type Description Default <code>nct_id</code> <code>str</code> <p>ClinicalTrials.gov identifier.</p> required <code>title</code> <code>str</code> <p>Trial title.</p> required <code>condition</code> <code>str</code> <p>Primary condition or disease area.</p> required <code>phase</code> <code>str</code> <p>Trial phase label.</p> required <code>document_text</code> <code>str</code> <p>Extracted protocol text for NLP processing.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ProtocolRecord(\n...     nct_id=\"NCT00000000\",\n...     title=\"Example Trial\",\n...     condition=\"Melanoma\",\n...     phase=\"Phase 2\",\n...     document_text=\"Inclusion: Age &gt;= 18.\",\n... )\nProtocolRecord(\n...     nct_id='NCT00000000',\n...     title='Example Trial',\n...     condition='Melanoma',\n...     phase='Phase 2',\n...     document_text='Inclusion: Age &gt;= 18.',\n... )\n</code></pre> Notes <p>This model represents the canonical ingestion output for the API service.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.ProtocolRecord-functions","title":"Functions","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.ProtocolRecord.to_protocol","title":"to_protocol","text":"<pre><code>to_protocol(protocol_id: str) -&gt; Protocol\n</code></pre> <p>Convert to shared Protocol model.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def to_protocol(self, protocol_id: str) -&gt; Protocol:\n    \"\"\"Convert to shared Protocol model.\"\"\"\n    return Protocol(\n        id=protocol_id,\n        title=self.title,\n        nct_id=self.nct_id,\n        condition=self.condition,\n        phase=self.phase,\n        source=self.source,\n        registry_id=self.registry_id,\n        registry_type=self.registry_type,\n    )\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.ProtocolRecord.to_document","title":"to_document","text":"<pre><code>to_document(doc_id: str, protocol_id: str) -&gt; Document\n</code></pre> <p>Convert to shared Document model.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def to_document(self, doc_id: str, protocol_id: str) -&gt; Document:\n    \"\"\"Convert to shared Document model.\"\"\"\n    return Document(\n        id=doc_id,\n        protocol_id=protocol_id,\n        text=self.document_text,\n        source_url=self.source_url,\n    )\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline-functions","title":"Functions","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.emit_records","title":"emit_records","text":"<pre><code>emit_records(records: list[ProtocolRecord], output_path: Path | None = None) -&gt; None\n</code></pre> <p>Write protocol records to JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[ProtocolRecord]</code> <p>Protocol records to emit.</p> required <code>output_path</code> <code>Path | None</code> <p>Output file path. If None, prints to stdout.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def emit_records(\n    records: list[ProtocolRecord], output_path: Path | None = None\n) -&gt; None:\n    \"\"\"Write protocol records to JSONL file.\n\n    Args:\n        records: Protocol records to emit.\n        output_path: Output file path. If None, prints to stdout.\n\n    Returns:\n        None.\n    \"\"\"\n    lines = [json.dumps(asdict(record)) for record in records]\n\n    if output_path:\n        try:\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n            output_path.write_text(\"\\n\".join(lines) + \"\\n\")\n        except OSError as exc:\n            message = f\"Failed to write output to {output_path}: {exc}\"\n            raise RuntimeError(message) from exc\n    else:\n        for line in lines:\n            print(line)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.extract_text_from_pdf","title":"extract_text_from_pdf","text":"<pre><code>extract_text_from_pdf(path: Path) -&gt; str\n</code></pre> <p>Extract text from a PDF file using pypdf.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def extract_text_from_pdf(path: Path) -&gt; str:\n    \"\"\"Extract text from a PDF file using pypdf.\"\"\"\n    reader = PdfReader(str(path))\n    chunks: list[str] = []\n    for page in reader.pages:\n        page_text = page.extract_text() or \"\"\n        if page_text:\n            chunks.append(page_text)\n    return \"\\n\".join(chunks).strip()\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.ingest_local_protocols","title":"ingest_local_protocols","text":"<pre><code>ingest_local_protocols(manifest_path: Path = DEFAULT_MANIFEST_PATH, limit: int = 50) -&gt; list[ProtocolRecord]\n</code></pre> <p>Load protocol PDFs referenced in a manifest and extract document text.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def ingest_local_protocols(\n    manifest_path: Path = DEFAULT_MANIFEST_PATH, limit: int = 50\n) -&gt; list[ProtocolRecord]:\n    \"\"\"Load protocol PDFs referenced in a manifest and extract document text.\"\"\"\n    if limit &lt;= 0:\n        raise ValueError(\"limit must be positive\")\n    if not manifest_path.exists():\n        raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n\n    records: list[ProtocolRecord] = []\n    for entry in read_manifest_entries(manifest_path):\n        if len(records) &gt;= limit:\n            break\n        record = _build_record_from_entry(entry)\n        if record is not None:\n            records.append(record)\n    return records\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.read_manifest_entries","title":"read_manifest_entries","text":"<pre><code>read_manifest_entries(manifest_path: Path) -&gt; list[dict[str, object]]\n</code></pre> <p>Read manifest entries from a JSONL file.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def read_manifest_entries(manifest_path: Path) -&gt; list[dict[str, object]]:\n    \"\"\"Read manifest entries from a JSONL file.\"\"\"\n    entries: list[dict[str, object]] = []\n    with manifest_path.open(encoding=\"utf-8\") as handle:\n        for line in handle:\n            if line.strip():\n                try:\n                    parsed = json.loads(line)\n                except json.JSONDecodeError as exc:\n                    logger.warning(\n                        \"Skipping malformed manifest line: %s (%s)\",\n                        line[:200].rstrip(),\n                        exc,\n                    )\n                    continue\n                if isinstance(parsed, dict):\n                    entries.append(parsed)\n                else:\n                    logger.warning(\n                        \"Skipping non-dict manifest entry: %r\",\n                        type(parsed),\n                    )\n    return entries\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.main","title":"main","text":"<pre><code>main() -&gt; int\n</code></pre> <p>Sync CLI entrypoint.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Sync CLI entrypoint.\"\"\"\n    return asyncio.run(main_async())\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.main_async","title":"main_async  <code>async</code>","text":"<pre><code>main_async() -&gt; int\n</code></pre> <p>Async CLI entrypoint for downloading protocol PDFs.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def main_async() -&gt; int:\n    \"\"\"Async CLI entrypoint for downloading protocol PDFs.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Download ONLY clinical trial protocol PDFs from specified sources.\"\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"data/protocols\",\n        help=\"Directory to store downloaded PDFs.\",\n    )\n    parser.add_argument(\n        \"--include-journal-sources\",\n        action=\"store_true\",\n        help=\"Include demoted journal sources (bmjopen, jmir).\",\n    )\n    parser.add_argument(\n        \"--sources\",\n        nargs=\"+\",\n        choices=sorted(SOURCE_SPECS.keys()),\n        default=None,\n        help=\"Which sources to download from.\",\n    )\n    parser.add_argument(\n        \"--max-per-source\",\n        type=int,\n        default=50,\n        help=\"Maximum number of PDFs to download per source.\",\n    )\n    parser.add_argument(\n        \"--max-total\",\n        type=int,\n        default=200,\n        help=\"Maximum number of PDFs to download overall.\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        default=30,\n        help=\"Network timeout in seconds.\",\n    )\n    parser.add_argument(\n        \"--sitemap-limit\",\n        type=int,\n        default=2,\n        help=\"Number of sitemap files to scan per source.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose (DEBUG) logging.\",\n    )\n\n    args = parser.parse_args()\n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        logger.setLevel(logging.DEBUG)\n        logger.info(\"Verbose logging enabled\")\n\n    config = build_config(args)\n    downloader = ProtocolDownloader(config)\n    return await downloader.run()\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.bulk_load_protocols","title":"bulk_load_protocols","text":"<pre><code>bulk_load_protocols(manifest_path: Path = DEFAULT_MANIFEST_PATH, api_url: str = 'http://localhost:8000', limit: int = 50, auto_extract: bool = False) -&gt; list[str]\n</code></pre> <p>Bulk load protocols from a manifest into the database.</p> <p>Parameters:</p> Name Type Description Default <code>manifest_path</code> <code>Path</code> <p>Manifest JSONL containing downloaded PDFs.</p> <code>DEFAULT_MANIFEST_PATH</code> <code>api_url</code> <code>str</code> <p>API base URL.</p> <code>'http://localhost:8000'</code> <code>limit</code> <code>int</code> <p>Max number of records to load.</p> <code>50</code> <code>auto_extract</code> <code>bool</code> <p>Trigger extraction for each protocol after creation.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of created protocol IDs.</p> Source code in <code>components/data-pipeline/src/data_pipeline/loader.py</code> <pre><code>def bulk_load_protocols(\n    manifest_path: Path = DEFAULT_MANIFEST_PATH,\n    api_url: str = \"http://localhost:8000\",\n    limit: int = 50,\n    auto_extract: bool = False,\n) -&gt; list[str]:\n    \"\"\"Bulk load protocols from a manifest into the database.\n\n    Args:\n        manifest_path: Manifest JSONL containing downloaded PDFs.\n        api_url: API base URL.\n        limit: Max number of records to load.\n        auto_extract: Trigger extraction for each protocol after creation.\n\n    Returns:\n        List of created protocol IDs.\n    \"\"\"\n    records = ingest_local_protocols(manifest_path, limit=limit)\n    protocol_ids: list[str] = []\n\n    for record in records:\n        response = httpx.post(\n            f\"{api_url.rstrip('/')}/v1/protocols\",\n            json=_record_payload(record),\n            timeout=30.0,\n        )\n        if response.status_code != 200:\n            logger.warning(\n                \"Failed to create protocol %s (%s)\",\n                record.title,\n                response.text,\n            )\n            continue\n        payload = cast(dict[str, str], response.json())\n        protocol_id = payload[\"protocol_id\"]\n        protocol_ids.append(protocol_id)\n\n        if auto_extract:\n            extract_resp = httpx.post(\n                f\"{api_url.rstrip('/')}/v1/protocols/{protocol_id}/extract\",\n                timeout=30.0,\n            )\n            if extract_resp.status_code != 200:\n                logger.warning(\n                    \"Failed to extract criteria for %s (%s)\",\n                    protocol_id,\n                    extract_resp.text,\n                )\n\n    return protocol_ids\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.load_single_protocol","title":"load_single_protocol","text":"<pre><code>load_single_protocol(pdf_path: Path, api_url: str, auto_extract: bool = True) -&gt; str\n</code></pre> <p>Load a single PDF protocol into the database via the API.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to a protocol PDF file.</p> required <code>api_url</code> <code>str</code> <p>API base URL (e.g., http://localhost:8000).</p> required <code>auto_extract</code> <code>bool</code> <p>Trigger criteria extraction after creating the protocol.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The created protocol_id.</p> Source code in <code>components/data-pipeline/src/data_pipeline/loader.py</code> <pre><code>def load_single_protocol(\n    pdf_path: Path,\n    api_url: str,\n    auto_extract: bool = True,\n) -&gt; str:\n    \"\"\"Load a single PDF protocol into the database via the API.\n\n    Args:\n        pdf_path: Path to a protocol PDF file.\n        api_url: API base URL (e.g., http://localhost:8000).\n        auto_extract: Trigger criteria extraction after creating the protocol.\n\n    Returns:\n        The created protocol_id.\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n\n    text = extract_text_from_pdf(pdf_path)\n    if not text:\n        raise ValueError(f\"No text extracted from {pdf_path}\")\n\n    title = _derive_title(pdf_path, text)\n    response = httpx.post(\n        f\"{api_url.rstrip('/')}/v1/protocols\",\n        json={\"title\": title, \"document_text\": text},\n        timeout=30.0,\n    )\n    response.raise_for_status()\n    payload = cast(dict[str, str], response.json())\n    protocol_id = payload[\"protocol_id\"]\n\n    if auto_extract:\n        extract_resp = httpx.post(\n            f\"{api_url.rstrip('/')}/v1/protocols/{protocol_id}/extract\",\n            timeout=30.0,\n        )\n        extract_resp.raise_for_status()\n\n    return protocol_id\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline-modules","title":"Modules","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols","title":"download_protocols","text":"<p>Download protocols and emit normalized records.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols-classes","title":"Classes","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.ProtocolRecord","title":"ProtocolRecord  <code>dataclass</code>","text":"<pre><code>ProtocolRecord(nct_id: str, title: str, condition: str, phase: str, document_text: str, source: str | None = None, registry_id: str | None = None, registry_type: str | None = None, source_url: str | None = None)\n</code></pre> <p>Normalized protocol record for downstream services.</p> <p>Parameters:</p> Name Type Description Default <code>nct_id</code> <code>str</code> <p>ClinicalTrials.gov identifier.</p> required <code>title</code> <code>str</code> <p>Trial title.</p> required <code>condition</code> <code>str</code> <p>Primary condition or disease area.</p> required <code>phase</code> <code>str</code> <p>Trial phase label.</p> required <code>document_text</code> <code>str</code> <p>Extracted protocol text for NLP processing.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ProtocolRecord(\n...     nct_id=\"NCT00000000\",\n...     title=\"Example Trial\",\n...     condition=\"Melanoma\",\n...     phase=\"Phase 2\",\n...     document_text=\"Inclusion: Age &gt;= 18.\",\n... )\nProtocolRecord(\n...     nct_id='NCT00000000',\n...     title='Example Trial',\n...     condition='Melanoma',\n...     phase='Phase 2',\n...     document_text='Inclusion: Age &gt;= 18.',\n... )\n</code></pre> Notes <p>This model represents the canonical ingestion output for the API service.</p> Functions to_protocol <pre><code>to_protocol(protocol_id: str) -&gt; Protocol\n</code></pre> <p>Convert to shared Protocol model.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def to_protocol(self, protocol_id: str) -&gt; Protocol:\n    \"\"\"Convert to shared Protocol model.\"\"\"\n    return Protocol(\n        id=protocol_id,\n        title=self.title,\n        nct_id=self.nct_id,\n        condition=self.condition,\n        phase=self.phase,\n        source=self.source,\n        registry_id=self.registry_id,\n        registry_type=self.registry_type,\n    )\n</code></pre> to_document <pre><code>to_document(doc_id: str, protocol_id: str) -&gt; Document\n</code></pre> <p>Convert to shared Document model.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def to_document(self, doc_id: str, protocol_id: str) -&gt; Document:\n    \"\"\"Convert to shared Document model.\"\"\"\n    return Document(\n        id=doc_id,\n        protocol_id=protocol_id,\n        text=self.document_text,\n        source_url=self.source_url,\n    )\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols-functions","title":"Functions","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.extract_text_from_pdf","title":"extract_text_from_pdf","text":"<pre><code>extract_text_from_pdf(path: Path) -&gt; str\n</code></pre> <p>Extract text from a PDF file using pypdf.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def extract_text_from_pdf(path: Path) -&gt; str:\n    \"\"\"Extract text from a PDF file using pypdf.\"\"\"\n    reader = PdfReader(str(path))\n    chunks: list[str] = []\n    for page in reader.pages:\n        page_text = page.extract_text() or \"\"\n        if page_text:\n            chunks.append(page_text)\n    return \"\\n\".join(chunks).strip()\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.read_manifest_entries","title":"read_manifest_entries","text":"<pre><code>read_manifest_entries(manifest_path: Path) -&gt; list[dict[str, object]]\n</code></pre> <p>Read manifest entries from a JSONL file.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def read_manifest_entries(manifest_path: Path) -&gt; list[dict[str, object]]:\n    \"\"\"Read manifest entries from a JSONL file.\"\"\"\n    entries: list[dict[str, object]] = []\n    with manifest_path.open(encoding=\"utf-8\") as handle:\n        for line in handle:\n            if line.strip():\n                try:\n                    parsed = json.loads(line)\n                except json.JSONDecodeError as exc:\n                    logger.warning(\n                        \"Skipping malformed manifest line: %s (%s)\",\n                        line[:200].rstrip(),\n                        exc,\n                    )\n                    continue\n                if isinstance(parsed, dict):\n                    entries.append(parsed)\n                else:\n                    logger.warning(\n                        \"Skipping non-dict manifest entry: %r\",\n                        type(parsed),\n                    )\n    return entries\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.ingest_local_protocols","title":"ingest_local_protocols","text":"<pre><code>ingest_local_protocols(manifest_path: Path = DEFAULT_MANIFEST_PATH, limit: int = 50) -&gt; list[ProtocolRecord]\n</code></pre> <p>Load protocol PDFs referenced in a manifest and extract document text.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def ingest_local_protocols(\n    manifest_path: Path = DEFAULT_MANIFEST_PATH, limit: int = 50\n) -&gt; list[ProtocolRecord]:\n    \"\"\"Load protocol PDFs referenced in a manifest and extract document text.\"\"\"\n    if limit &lt;= 0:\n        raise ValueError(\"limit must be positive\")\n    if not manifest_path.exists():\n        raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n\n    records: list[ProtocolRecord] = []\n    for entry in read_manifest_entries(manifest_path):\n        if len(records) &gt;= limit:\n            break\n        record = _build_record_from_entry(entry)\n        if record is not None:\n            records.append(record)\n    return records\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.emit_records","title":"emit_records","text":"<pre><code>emit_records(records: list[ProtocolRecord], output_path: Path | None = None) -&gt; None\n</code></pre> <p>Write protocol records to JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[ProtocolRecord]</code> <p>Protocol records to emit.</p> required <code>output_path</code> <code>Path | None</code> <p>Output file path. If None, prints to stdout.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def emit_records(\n    records: list[ProtocolRecord], output_path: Path | None = None\n) -&gt; None:\n    \"\"\"Write protocol records to JSONL file.\n\n    Args:\n        records: Protocol records to emit.\n        output_path: Output file path. If None, prints to stdout.\n\n    Returns:\n        None.\n    \"\"\"\n    lines = [json.dumps(asdict(record)) for record in records]\n\n    if output_path:\n        try:\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n            output_path.write_text(\"\\n\".join(lines) + \"\\n\")\n        except OSError as exc:\n            message = f\"Failed to write output to {output_path}: {exc}\"\n            raise RuntimeError(message) from exc\n    else:\n        for line in lines:\n            print(line)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.download_protocols.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>CLI entrypoint.</p> Source code in <code>components/data-pipeline/src/data_pipeline/download_protocols.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"CLI entrypoint.\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Ingest protocols from local PDFs using manifest.jsonl\"\n    )\n    parser.add_argument(\n        \"--manifest-path\",\n        type=Path,\n        default=DEFAULT_MANIFEST_PATH,\n        help=\"Path to manifest.jsonl for local ingestion\",\n    )\n    parser.add_argument(\"--limit\", type=int, default=50, help=\"Max records\")\n    parser.add_argument(\"--output\", type=Path, help=\"Output JSONL path\")\n    args = parser.parse_args()\n\n    records = ingest_local_protocols(args.manifest_path, args.limit)\n    emit_records(records, args.output)\n    print(f\"Ingested {len(records)} protocols\")\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader","title":"downloader","text":"<p>Download ONLY clinical trial protocols from designated sources.</p> <p>This module downloads protocol PDFs from public sources with strict filtering to exclude amendments, deviations, violations, and unrelated documents.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader-classes","title":"Classes","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.SourceSpec","title":"SourceSpec  <code>dataclass</code>","text":"<pre><code>SourceSpec(name: str, discovery_method: str, identifier_type: str, priority: str, enabled_by_default: bool)\n</code></pre> <p>Metadata for a protocol source.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.DownloadConfig","title":"DownloadConfig  <code>dataclass</code>","text":"<pre><code>DownloadConfig(output_dir: Path, include_journal_sources: bool, sources: list[str] | None, max_per_source: int, max_total: int, timeout: int, sitemap_limit: int, verbose: bool)\n</code></pre> <p>Configuration for the download run.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.LinkExtractor","title":"LinkExtractor","text":"<pre><code>LinkExtractor()\n</code></pre> <p>               Bases: <code>HTMLParser</code></p> <pre><code>\n              flowchart TD\n              data_pipeline.downloader.LinkExtractor[LinkExtractor]\n\n              \n\n              click data_pipeline.downloader.LinkExtractor href \"\" \"data_pipeline.downloader.LinkExtractor\"\n            </code></pre> <p>Collect anchors and meta tags from an HTML document.</p> <p>Initialize an HTML link extractor.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an HTML link extractor.\"\"\"\n    super().__init__()\n    self.links: list[str] = []\n    self.meta: dict[str, str] = {}\n    self._current_href: Optional[str] = None\n    self._current_text: list[str] = []\n    self.link_text: dict[str, str] = {}\n</code></pre> Functions handle_starttag <pre><code>handle_starttag(tag: str, attrs: list[tuple[str, str | None]]) -&gt; None\n</code></pre> <p>Track anchor and meta tags.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -&gt; None:\n    \"\"\"Track anchor and meta tags.\"\"\"\n    attrs_dict = dict(attrs)\n    if tag == \"a\":\n        href = attrs_dict.get(\"href\")\n        if href:\n            self.links.append(href)\n            self._current_href = href\n            self._current_text = []\n    if tag == \"meta\":\n        name = attrs_dict.get(\"name\") or attrs_dict.get(\"property\")\n        content = attrs_dict.get(\"content\")\n        if name and content:\n            self.meta[name.lower()] = content\n</code></pre> handle_endtag <pre><code>handle_endtag(tag: str) -&gt; None\n</code></pre> <p>Capture anchor text after closing tags.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def handle_endtag(self, tag: str) -&gt; None:\n    \"\"\"Capture anchor text after closing tags.\"\"\"\n    if tag == \"a\" and self._current_href:\n        text = \" \".join(self._current_text).strip()\n        if text:\n            self.link_text[self._current_href] = text\n        self._current_href = None\n        self._current_text = []\n</code></pre> handle_data <pre><code>handle_data(data: str) -&gt; None\n</code></pre> <p>Collect text inside anchors.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def handle_data(self, data: str) -&gt; None:\n    \"\"\"Collect text inside anchors.\"\"\"\n    if self._current_href:\n        self._current_text.append(data)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.ProtocolDownloader","title":"ProtocolDownloader","text":"<pre><code>ProtocolDownloader(config: DownloadConfig)\n</code></pre> <p>Downloader orchestration for protocol sources.</p> <p>Initialize downloader with config.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def __init__(self, config: DownloadConfig) -&gt; None:\n    \"\"\"Initialize downloader with config.\"\"\"\n    self.config = config\n    self.manifest_lock = asyncio.Lock()\n    self.semaphore = asyncio.Semaphore(compute_concurrency_limit())\n</code></pre> Functions run <code>async</code> <pre><code>run() -&gt; int\n</code></pre> <p>Run the download pipeline across selected sources.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def run(self) -&gt; int:\n    \"\"\"Run the download pipeline across selected sources.\"\"\"\n    output_dir = self.config.output_dir\n    ensure_dir(output_dir)\n    ensure_dir(output_dir / \"crc_protocols\")\n    ensure_dir(output_dir / \"protocol_papers\")\n\n    manifest_path = output_dir / \"manifest.jsonl\"\n    total_downloaded = 0\n    source_results: dict[str, int] = {}\n\n    async with aiohttp.ClientSession(\n        headers={\"User-Agent\": USER_AGENT},\n        connector=aiohttp.TCPConnector(ssl=SSL_CONTEXT),\n        timeout=aiohttp.ClientTimeout(total=self.config.timeout),\n    ) as session:\n        for source in self._selected_sources():\n            if total_downloaded &gt;= self.config.max_total:\n                logger.info(\"Reached max_total (%s)\", self.config.max_total)\n                break\n\n            handler = self._source_handlers()[source]\n            remaining = self.config.max_total - total_downloaded\n            per_source_limit = min(self.config.max_per_source, remaining)\n            logger.info(\n                \"Processing: %s (target: %s PDFs)\", source, per_source_limit\n            )\n\n            downloaded = await handler(\n                session=session,\n                manifest_path=manifest_path,\n                max_items=per_source_limit,\n            )\n            source_results[source] = downloaded\n            total_downloaded += downloaded\n            logger.info(\n                \"%s: %s PDFs (total: %s)\",\n                source,\n                downloaded,\n                total_downloaded,\n            )\n\n            await asyncio.sleep(1)\n\n    self._log_summary(total_downloaded, source_results, manifest_path)\n    return total_downloaded\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader-functions","title":"Functions","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.fetch_url","title":"fetch_url  <code>async</code>","text":"<pre><code>fetch_url(url: str, *, session: ClientSession, semaphore: Semaphore, timeout: int = 30) -&gt; bytes\n</code></pre> <p>Fetch bytes from a URL with retry logic.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS)\n    | retry_if_exception(_is_retryable_http_error),\n    reraise=True,\n)\nasync def fetch_url(\n    url: str,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    timeout: int = 30,\n) -&gt; bytes:\n    \"\"\"Fetch bytes from a URL with retry logic.\"\"\"\n    async with semaphore:\n        async with session.get(\n            url,\n            timeout=aiohttp.ClientTimeout(total=timeout),\n            ssl=SSL_CONTEXT,\n        ) as response:\n            if response.status &gt;= 400:\n                response.raise_for_status()\n            return await response.read()\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.fetch_json","title":"fetch_json  <code>async</code>","text":"<pre><code>fetch_json(url: str, *, session: ClientSession, semaphore: Semaphore, params: Optional[dict[str, str]] = None, timeout: int = 30) -&gt; JsonDict\n</code></pre> <p>Fetch JSON from a URL with retry logic.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS)\n    | retry_if_exception(_is_retryable_http_error),\n    reraise=True,\n)\nasync def fetch_json(\n    url: str,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    params: Optional[dict[str, str]] = None,\n    timeout: int = 30,\n) -&gt; JsonDict:\n    \"\"\"Fetch JSON from a URL with retry logic.\"\"\"\n    if params:\n        query_string = urllib.parse.urlencode(params)\n        url = f\"{url}?{query_string}\"\n    data = await fetch_url(url, session=session, semaphore=semaphore, timeout=timeout)\n    return cast(JsonDict, json.loads(data.decode(\"utf-8\")))\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.fetch_json_post","title":"fetch_json_post  <code>async</code>","text":"<pre><code>fetch_json_post(url: str, *, session: ClientSession, semaphore: Semaphore, payload: JsonDict, timeout: int = 30) -&gt; JsonDict\n</code></pre> <p>POST JSON and return JSON response with retry logic.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS)\n    | retry_if_exception(_is_retryable_http_error),\n    reraise=True,\n)\nasync def fetch_json_post(\n    url: str,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    payload: JsonDict,\n    timeout: int = 30,\n) -&gt; JsonDict:\n    \"\"\"POST JSON and return JSON response with retry logic.\"\"\"\n    async with semaphore:\n        async with session.post(\n            url,\n            json=payload,\n            timeout=aiohttp.ClientTimeout(total=timeout),\n            ssl=SSL_CONTEXT,\n        ) as response:\n            if response.status &gt;= 400:\n                response.raise_for_status()\n            data = await response.read()\n            return cast(JsonDict, json.loads(data.decode(\"utf-8\")))\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.normalize_filename","title":"normalize_filename","text":"<pre><code>normalize_filename(url: str, suffix: str = '.pdf') -&gt; str\n</code></pre> <p>Generate a filesystem-safe filename for a URL.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def normalize_filename(url: str, suffix: str = \".pdf\") -&gt; str:\n    \"\"\"Generate a filesystem-safe filename for a URL.\"\"\"\n    parsed = urllib.parse.urlparse(url)\n    basename = Path(parsed.path).name or \"document\"\n    stem = Path(basename).stem or \"document\"\n    safe_stem = re.sub(r\"[^A-Za-z0-9._-]+\", \"-\", stem).strip(\"-\") or \"document\"\n    short_hash = hashlib.sha256(url.encode(\"utf-8\")).hexdigest()[:8]\n    ext = Path(basename).suffix or suffix\n    if not ext.startswith(\".\"):\n        ext = f\".{ext}\"\n    return f\"{safe_stem}-{short_hash}{ext}\"\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.ensure_dir","title":"ensure_dir","text":"<pre><code>ensure_dir(path: Path) -&gt; None\n</code></pre> <p>Create a directory if it does not exist.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def ensure_dir(path: Path) -&gt; None:\n    \"\"\"Create a directory if it does not exist.\"\"\"\n    path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.compute_concurrency_limit","title":"compute_concurrency_limit","text":"<pre><code>compute_concurrency_limit() -&gt; int\n</code></pre> <p>Compute a reasonable concurrency limit for downloads.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def compute_concurrency_limit() -&gt; int:\n    \"\"\"Compute a reasonable concurrency limit for downloads.\"\"\"\n    cpu_count = os.cpu_count() or 1\n    return min(32, max(1, cpu_count * 2))\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.parse_html_links","title":"parse_html_links","text":"<pre><code>parse_html_links(html: bytes, base_url: str) -&gt; tuple[Set[str], dict[str, str], dict[str, str]]\n</code></pre> <p>Parse links and meta tags from HTML content.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def parse_html_links(\n    html: bytes, base_url: str\n) -&gt; tuple[Set[str], dict[str, str], dict[str, str]]:\n    \"\"\"Parse links and meta tags from HTML content.\"\"\"\n    parser = LinkExtractor()\n    parser.feed(html.decode(\"utf-8\", errors=\"ignore\"))\n    links = {urllib.parse.urljoin(base_url, link) for link in parser.links}\n    link_text = {\n        urllib.parse.urljoin(base_url, href): text\n        for href, text in parser.link_text.items()\n    }\n    return links, parser.meta, link_text\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.extract_pdf_links","title":"extract_pdf_links","text":"<pre><code>extract_pdf_links(html: bytes, base_url: str) -&gt; list[str]\n</code></pre> <p>Extract PDF links from HTML content.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def extract_pdf_links(html: bytes, base_url: str) -&gt; list[str]:\n    \"\"\"Extract PDF links from HTML content.\"\"\"\n    links, meta, _ = parse_html_links(html, base_url)\n    pdf_links: Set[str] = set()\n    citation_pdf = meta.get(\"citation_pdf_url\")\n    if citation_pdf:\n        pdf_links.add(urllib.parse.urljoin(base_url, citation_pdf))\n    for link in links:\n        if \".pdf\" in link.lower():\n            pdf_links.add(link)\n    return sorted(pdf_links)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.is_same_domain","title":"is_same_domain","text":"<pre><code>is_same_domain(url: str, base_url: str) -&gt; bool\n</code></pre> <p>Check if a URL shares the same domain as base_url.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def is_same_domain(url: str, base_url: str) -&gt; bool:\n    \"\"\"Check if a URL shares the same domain as base_url.\"\"\"\n    return _normalize_domain(url) == _normalize_domain(base_url)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.find_pdf_links","title":"find_pdf_links","text":"<pre><code>find_pdf_links(html: bytes, base_url: str, *, include_keywords: Optional[Set[str]] = None) -&gt; list[str]\n</code></pre> <p>Find PDF links in HTML, optionally filtered by keyword.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def find_pdf_links(\n    html: bytes,\n    base_url: str,\n    *,\n    include_keywords: Optional[Set[str]] = None,\n) -&gt; list[str]:\n    \"\"\"Find PDF links in HTML, optionally filtered by keyword.\"\"\"\n    links, meta, link_text = parse_html_links(html, base_url)\n    pdf_urls: Set[str] = set()\n    citation_pdf = meta.get(\"citation_pdf_url\")\n    if citation_pdf:\n        pdf_urls.add(urllib.parse.urljoin(base_url, citation_pdf))\n    for link in links:\n        lowered = link.lower()\n        if \".pdf\" in lowered or \"/pdf\" in lowered or \"download\" in lowered:\n            pdf_urls.add(link)\n    if include_keywords:\n        filtered = set()\n        for link in pdf_urls:\n            text = link_text.get(link, \"\").lower()\n            if any(keyword in text for keyword in include_keywords) or any(\n                keyword in link.lower() for keyword in include_keywords\n            ):\n                filtered.add(link)\n        if filtered:\n            pdf_urls = filtered\n    return [url for url in sorted(pdf_urls) if is_same_domain(url, base_url)]\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.extract_isrctn_ids","title":"extract_isrctn_ids","text":"<pre><code>extract_isrctn_ids(xml_data: bytes) -&gt; list[str]\n</code></pre> <p>Extract ISRCTN identifiers from an XML payload.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def extract_isrctn_ids(xml_data: bytes) -&gt; list[str]:\n    \"\"\"Extract ISRCTN identifiers from an XML payload.\"\"\"\n    ids: Set[str] = set()\n    try:\n        root = ET.fromstring(xml_data)\n    except ET.ParseError:\n        return []\n    for node in root.iter():\n        tag = node.tag.lower()\n        if (tag.endswith(\"isrctnid\") or tag.endswith(\"isrctn\")) and node.text:\n            raw = node.text.strip()\n            if raw.upper().startswith(\"ISRCTN\"):\n                ids.add(raw.upper())\n            elif raw.isdigit():\n                ids.add(f\"ISRCTN{raw}\")\n    return sorted(ids)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.extract_isrctn_protocol_files","title":"extract_isrctn_protocol_files","text":"<pre><code>extract_isrctn_protocol_files(xml_data: bytes) -&gt; list[tuple[str, str, str]]\n</code></pre> <p>Extract protocol files from ISRCTN XML payloads.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def extract_isrctn_protocol_files(xml_data: bytes) -&gt; list[tuple[str, str, str]]:\n    \"\"\"Extract protocol files from ISRCTN XML payloads.\"\"\"\n    try:\n        root = ET.fromstring(xml_data)\n    except ET.ParseError:\n        return []\n    namespace = \"\"\n    if root.tag.startswith(\"{\"):\n        namespace = root.tag.split(\"}\")[0].strip(\"{\")\n    ns = {\"i\": namespace} if namespace else {}\n    results: list[tuple[str, str, str]] = []\n\n    for trial in root.findall(\".//i:trial\", ns) if ns else root.iter(\"trial\"):\n        isrctn_node = trial.find(\".//i:isrctn\", ns) if ns else trial.find(\".//isrctn\")\n        if isrctn_node is None or not isrctn_node.text:\n            continue\n        isrctn_id = f\"ISRCTN{isrctn_node.text.strip()}\"\n        attached_files = (\n            trial.findall(\".//i:attachedFile\", ns)\n            if ns\n            else trial.findall(\".//attachedFile\")\n        )\n        for file_node in attached_files:\n            download_url = file_node.attrib.get(\"downloadUrl\", \"\")\n            description_node = (\n                file_node.find(\"i:description\", ns)\n                if ns\n                else file_node.find(\"description\")\n            )\n            description = (\n                description_node.text.strip()\n                if description_node is not None and description_node.text\n                else \"\"\n            )\n            if \"protocol\" in description.lower() and download_url:\n                results.append((isrctn_id, download_url, description))\n\n    return results\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.extract_ctis_protocol_links","title":"extract_ctis_protocol_links","text":"<pre><code>extract_ctis_protocol_links(payload: object) -&gt; list[tuple[str, str]]\n</code></pre> <p>Extract protocol document links from CTIS payloads.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def extract_ctis_protocol_links(payload: object) -&gt; list[tuple[str, str]]:\n    \"\"\"Extract protocol document links from CTIS payloads.\"\"\"\n    protocol_links: list[tuple[str, str]] = []\n    for url_value, label in _collect_ctis_links(payload):\n        lowered = label.lower()\n        if \"protocol\" in lowered and \"synopsis\" not in lowered and \"icf\" not in lowered:\n            protocol_links.append((url_value, label))\n    return protocol_links\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.looks_like_protocol_text","title":"looks_like_protocol_text","text":"<pre><code>looks_like_protocol_text(text: str) -&gt; bool\n</code></pre> <p>Heuristic check for protocol-related text.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def looks_like_protocol_text(text: str) -&gt; bool:\n    \"\"\"Heuristic check for protocol-related text.\"\"\"\n    lowered = text.lower()\n    if \"protocol\" not in lowered and \"study protocol\" not in lowered:\n        return False\n    if \"statistical analysis plan\" in lowered or \"sap\" in lowered:\n        return False\n    return True\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.validate_protocol_pdf_content","title":"validate_protocol_pdf_content","text":"<pre><code>validate_protocol_pdf_content(data: bytes) -&gt; Optional[bool]\n</code></pre> <p>Inspect PDF content for protocol indicators when available.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def validate_protocol_pdf_content(data: bytes) -&gt; Optional[bool]:\n    \"\"\"Inspect PDF content for protocol indicators when available.\"\"\"\n    try:\n        from pypdf import PdfReader\n    except Exception:\n        logger.debug(\"pypdf not available; skipping content validation\")\n        return None\n\n    try:\n        from io import BytesIO\n\n        reader = PdfReader(BytesIO(data))\n        text_chunks: list[str] = []\n        for page in reader.pages[:2]:\n            extracted = page.extract_text() or \"\"\n            if extracted:\n                text_chunks.append(extracted)\n        text = \" \".join(text_chunks).strip()\n        if len(text) &lt; 200:\n            logger.debug(\"PDF text extraction too sparse; skipping content validation\")\n            return None\n        return looks_like_protocol_text(text)\n    except Exception as exc:\n        logger.debug(\"Failed to extract PDF text: %s\", exc)\n        return None\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.record_manifest_async","title":"record_manifest_async  <code>async</code>","text":"<pre><code>record_manifest_async(manifest_path: Path, source: str, url: str, path: Path, *, status: str, detail: Optional[str] = None, registry_id: Optional[str] = None, registry_type: Optional[str] = None, document_type: Optional[str] = None, lock: Lock) -&gt; None\n</code></pre> <p>Record a manifest entry asynchronously.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def record_manifest_async(\n    manifest_path: Path,\n    source: str,\n    url: str,\n    path: Path,\n    *,\n    status: str,\n    detail: Optional[str] = None,\n    registry_id: Optional[str] = None,\n    registry_type: Optional[str] = None,\n    document_type: Optional[str] = None,\n    lock: asyncio.Lock,\n) -&gt; None:\n    \"\"\"Record a manifest entry asynchronously.\"\"\"\n    async with lock:\n        await asyncio.to_thread(\n            record_manifest,\n            manifest_path,\n            source,\n            url,\n            path,\n            status=status,\n            detail=detail,\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n        )\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.record_manifest","title":"record_manifest","text":"<pre><code>record_manifest(manifest_path: Path, source: str, url: str, path: Path, *, status: str, detail: Optional[str] = None, registry_id: Optional[str] = None, registry_type: Optional[str] = None, document_type: Optional[str] = None) -&gt; None\n</code></pre> <p>Record a manifest entry to JSONL.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def record_manifest(\n    manifest_path: Path,\n    source: str,\n    url: str,\n    path: Path,\n    *,\n    status: str,\n    detail: Optional[str] = None,\n    registry_id: Optional[str] = None,\n    registry_type: Optional[str] = None,\n    document_type: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Record a manifest entry to JSONL.\"\"\"\n    record = {\n        \"timestamp\": dt.datetime.now(dt.timezone.utc)\n        .isoformat()\n        .replace(\"+00:00\", \"Z\"),\n        \"source\": source,\n        \"url\": url,\n        \"path\": str(path),\n        \"status\": status,\n    }\n    if detail:\n        record[\"detail\"] = detail\n    if registry_id:\n        record[\"registry_id\"] = registry_id\n    if registry_type:\n        record[\"registry_type\"] = registry_type\n    if document_type:\n        record[\"document_type\"] = document_type\n    with manifest_path.open(\"a\", encoding=\"utf-8\") as handle:\n        handle.write(json.dumps(record, ensure_ascii=True) + \"\\n\")\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.download_pdf","title":"download_pdf  <code>async</code>","text":"<pre><code>download_pdf(url: str, destination_dir: Path, *, session: ClientSession, semaphore: Semaphore, timeout: int, manifest_path: Path, source: str, manifest_lock: Lock, require_protocol: bool = False, registry_id: Optional[str] = None, registry_type: Optional[str] = None, document_type: Optional[str] = None) -&gt; TaskResult\n</code></pre> <p>Download a PDF file with validation and manifest logging.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def download_pdf(\n    url: str,\n    destination_dir: Path,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    timeout: int,\n    manifest_path: Path,\n    source: str,\n    manifest_lock: asyncio.Lock,\n    require_protocol: bool = False,\n    registry_id: Optional[str] = None,\n    registry_type: Optional[str] = None,\n    document_type: Optional[str] = None,\n) -&gt; TaskResult:\n    \"\"\"Download a PDF file with validation and manifest logging.\"\"\"\n    ensure_dir(destination_dir)\n    filename = normalize_filename(url)\n    target = destination_dir / filename\n\n    if target.exists():\n        logger.debug(\"File already exists: %s\", target)\n        return target\n\n    try:\n        data = await fetch_url(\n            url,\n            session=session,\n            semaphore=semaphore,\n            timeout=timeout,\n        )\n    except RetryError:\n        await record_manifest_async(\n            manifest_path,\n            source,\n            url,\n            target,\n            status=\"failed\",\n            detail=\"Retry exhausted\",\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n            lock=manifest_lock,\n        )\n        return None\n    except aiohttp.ClientResponseError as exc:\n        await record_manifest_async(\n            manifest_path,\n            source,\n            url,\n            target,\n            status=\"failed\",\n            detail=f\"HTTP {exc.status}\",\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n            lock=manifest_lock,\n        )\n        return None\n    except (aiohttp.ClientError, TimeoutError, ValueError, OSError) as exc:\n        await record_manifest_async(\n            manifest_path,\n            source,\n            url,\n            target,\n            status=\"failed\",\n            detail=str(exc),\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n            lock=manifest_lock,\n        )\n        return None\n\n    detail = _pdf_error_detail(data, require_protocol)\n    if detail:\n        await record_manifest_async(\n            manifest_path,\n            source,\n            url,\n            target,\n            status=\"failed\",\n            detail=detail,\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n            lock=manifest_lock,\n        )\n        return None\n\n    write_error = await _write_pdf(target, data)\n    if write_error:\n        await record_manifest_async(\n            manifest_path,\n            source,\n            url,\n            target,\n            status=\"failed\",\n            detail=write_error,\n            registry_id=registry_id,\n            registry_type=registry_type,\n            document_type=document_type,\n            lock=manifest_lock,\n        )\n        return None\n\n    await record_manifest_async(\n        manifest_path,\n        source,\n        url,\n        target,\n        status=\"downloaded\",\n        registry_id=registry_id,\n        registry_type=registry_type,\n        document_type=document_type,\n        lock=manifest_lock,\n    )\n    logger.info(\"Downloaded: %s (%s bytes)\", target.name, len(data))\n    return target\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.iter_protocol_docs","title":"iter_protocol_docs","text":"<pre><code>iter_protocol_docs(large_docs: Iterable[JsonDict]) -&gt; Iterator[JsonDict]\n</code></pre> <p>Yield actual study protocols, filtering out amendments/deviations.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def iter_protocol_docs(large_docs: Iterable[JsonDict]) -&gt; Iterator[JsonDict]:\n    \"\"\"Yield actual study protocols, filtering out amendments/deviations.\"\"\"\n    exclude_terms = {\"SAP\", \"ICF\", \"AMENDMENT\", \"DEVIATION\", \"VIOLATION\", \"CASE\"}\n\n    for doc in large_docs:\n        filename = str(doc.get(\"filename\") or \"\")\n        if not filename:\n            continue\n        filename_upper = filename.upper()\n\n        type_abbrev = str(doc.get(\"typeAbbrev\") or \"\").upper()\n        type_full = str(doc.get(\"type\") or \"\").upper()\n        label = str(doc.get(\"label\") or \"\").upper()\n\n        if any(\n            exclude in filename_upper\n            or exclude in type_abbrev\n            or exclude in type_full\n            or exclude in label\n            for exclude in exclude_terms\n        ):\n            logger.debug(\n                \"Excluding non-protocol doc: %s (type=%s/%s, label=%s)\",\n                filename,\n                type_abbrev,\n                type_full,\n                label,\n            )\n            continue\n\n        if doc.get(\"hasProtocol\") is True:\n            yield doc\n            continue\n\n        if \"PROTOCOL\" in type_full or \"STUDY PROTOCOL\" in label:\n            if \"AMENDMENT\" not in type_full and \"DEVIATION\" not in type_full:\n                yield doc\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.resolve_output_dir","title":"resolve_output_dir","text":"<pre><code>resolve_output_dir(output_root: Path, source: str) -&gt; Path\n</code></pre> <p>Resolve the output directory for a source.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def resolve_output_dir(output_root: Path, source: str) -&gt; Path:\n    \"\"\"Resolve the output directory for a source.\"\"\"\n    spec = SOURCE_SPECS.get(source)\n    if spec and spec.priority == \"secondary\":\n        return output_root / \"protocol_papers\" / source\n    return output_root / \"crc_protocols\" / source\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.iter_sitemap_urls","title":"iter_sitemap_urls  <code>async</code>","text":"<pre><code>iter_sitemap_urls(root_sitemap: str, *, session: ClientSession, semaphore: Semaphore, sitemap_limit: int, url_limit: Optional[int], timeout: int) -&gt; AsyncIterator[str]\n</code></pre> <p>Iterate over URLs from a sitemap hierarchy.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def iter_sitemap_urls(\n    root_sitemap: str,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    sitemap_limit: int,\n    url_limit: Optional[int],\n    timeout: int,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Iterate over URLs from a sitemap hierarchy.\"\"\"\n    logger.info(\"Fetching root sitemap: %s\", root_sitemap)\n    queue = await read_sitemap(\n        root_sitemap,\n        session=session,\n        semaphore=semaphore,\n        timeout=timeout,\n    )\n\n    if not queue:\n        logger.warning(\"Empty sitemap: %s\", root_sitemap)\n        return\n\n    sub_sitemaps = [\n        url\n        for url in queue\n        if url.endswith((\".xml\", \".xml.gz\")) or \"sitemap\" in url.lower()\n    ]\n\n    urls_yielded = 0\n    if sub_sitemaps:\n        for sitemap_url in sub_sitemaps[:sitemap_limit]:\n            try:\n                sitemap_urls = await read_sitemap(\n                    sitemap_url,\n                    session=session,\n                    semaphore=semaphore,\n                    timeout=timeout,\n                )\n            except (aiohttp.ClientError, ET.ParseError, RetryError) as exc:\n                logger.warning(\n                    \"Skipping sub-sitemap %s: %s\", sitemap_url, type(exc).__name__\n                )\n                continue\n            for url in sitemap_urls:\n                yield url\n                urls_yielded += 1\n                if url_limit is not None:\n                    url_limit -= 1\n                    if url_limit &lt;= 0:\n                        logger.info(\n                            \"Reached URL limit after yielding %s URLs\", urls_yielded\n                        )\n                        return\n    else:\n        limit = url_limit or len(queue)\n        logger.info(\"No sub-sitemaps found, yielding %s URLs\", min(limit, len(queue)))\n        for url in queue[:limit]:\n            yield url\n            urls_yielded += 1\n\n    logger.info(\"Sitemap iteration complete: yielded %s URLs\", urls_yielded)\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.read_sitemap","title":"read_sitemap  <code>async</code>","text":"<pre><code>read_sitemap(url: str, *, session: ClientSession, semaphore: Semaphore, timeout: int = 30) -&gt; list[str]\n</code></pre> <p>Read and parse a sitemap XML file.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def read_sitemap(\n    url: str,\n    *,\n    session: aiohttp.ClientSession,\n    semaphore: asyncio.Semaphore,\n    timeout: int = 30,\n) -&gt; list[str]:\n    \"\"\"Read and parse a sitemap XML file.\"\"\"\n    raw = await fetch_url(url, session=session, semaphore=semaphore, timeout=timeout)\n    if url.endswith(\".gz\"):\n        raw = gzip.decompress(raw)\n    root = ET.fromstring(raw)\n    ns = {\"s\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n    urls = []\n    for loc in root.findall(\".//s:loc\", ns):\n        if loc.text:\n            urls.append(loc.text.strip())\n    if not urls:\n        for loc in root.iter(\"loc\"):\n            if loc.text:\n                urls.append(loc.text.strip())\n    return urls\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.build_config","title":"build_config","text":"<pre><code>build_config(args: Namespace) -&gt; DownloadConfig\n</code></pre> <p>Build a DownloadConfig from CLI args.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def build_config(args: argparse.Namespace) -&gt; DownloadConfig:\n    \"\"\"Build a DownloadConfig from CLI args.\"\"\"\n    return DownloadConfig(\n        output_dir=Path(args.output_dir).resolve(),\n        include_journal_sources=args.include_journal_sources,\n        sources=args.sources,\n        max_per_source=args.max_per_source,\n        max_total=args.max_total,\n        timeout=args.timeout,\n        sitemap_limit=args.sitemap_limit,\n        verbose=args.verbose,\n    )\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.main_async","title":"main_async  <code>async</code>","text":"<pre><code>main_async() -&gt; int\n</code></pre> <p>Async CLI entrypoint for downloading protocol PDFs.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>async def main_async() -&gt; int:\n    \"\"\"Async CLI entrypoint for downloading protocol PDFs.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Download ONLY clinical trial protocol PDFs from specified sources.\"\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"data/protocols\",\n        help=\"Directory to store downloaded PDFs.\",\n    )\n    parser.add_argument(\n        \"--include-journal-sources\",\n        action=\"store_true\",\n        help=\"Include demoted journal sources (bmjopen, jmir).\",\n    )\n    parser.add_argument(\n        \"--sources\",\n        nargs=\"+\",\n        choices=sorted(SOURCE_SPECS.keys()),\n        default=None,\n        help=\"Which sources to download from.\",\n    )\n    parser.add_argument(\n        \"--max-per-source\",\n        type=int,\n        default=50,\n        help=\"Maximum number of PDFs to download per source.\",\n    )\n    parser.add_argument(\n        \"--max-total\",\n        type=int,\n        default=200,\n        help=\"Maximum number of PDFs to download overall.\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        default=30,\n        help=\"Network timeout in seconds.\",\n    )\n    parser.add_argument(\n        \"--sitemap-limit\",\n        type=int,\n        default=2,\n        help=\"Number of sitemap files to scan per source.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose (DEBUG) logging.\",\n    )\n\n    args = parser.parse_args()\n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        logger.setLevel(logging.DEBUG)\n        logger.info(\"Verbose logging enabled\")\n\n    config = build_config(args)\n    downloader = ProtocolDownloader(config)\n    return await downloader.run()\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.downloader.main","title":"main","text":"<pre><code>main() -&gt; int\n</code></pre> <p>Sync CLI entrypoint.</p> Source code in <code>components/data-pipeline/src/data_pipeline/downloader.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Sync CLI entrypoint.\"\"\"\n    return asyncio.run(main_async())\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.loader","title":"loader","text":"<p>Load protocols into the API database from PDFs or manifests.</p>"},{"location":"data-pipeline/docs/api/#data_pipeline.loader-classes","title":"Classes","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.loader-functions","title":"Functions","text":""},{"location":"data-pipeline/docs/api/#data_pipeline.loader.load_single_protocol","title":"load_single_protocol","text":"<pre><code>load_single_protocol(pdf_path: Path, api_url: str, auto_extract: bool = True) -&gt; str\n</code></pre> <p>Load a single PDF protocol into the database via the API.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to a protocol PDF file.</p> required <code>api_url</code> <code>str</code> <p>API base URL (e.g., http://localhost:8000).</p> required <code>auto_extract</code> <code>bool</code> <p>Trigger criteria extraction after creating the protocol.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The created protocol_id.</p> Source code in <code>components/data-pipeline/src/data_pipeline/loader.py</code> <pre><code>def load_single_protocol(\n    pdf_path: Path,\n    api_url: str,\n    auto_extract: bool = True,\n) -&gt; str:\n    \"\"\"Load a single PDF protocol into the database via the API.\n\n    Args:\n        pdf_path: Path to a protocol PDF file.\n        api_url: API base URL (e.g., http://localhost:8000).\n        auto_extract: Trigger criteria extraction after creating the protocol.\n\n    Returns:\n        The created protocol_id.\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n\n    text = extract_text_from_pdf(pdf_path)\n    if not text:\n        raise ValueError(f\"No text extracted from {pdf_path}\")\n\n    title = _derive_title(pdf_path, text)\n    response = httpx.post(\n        f\"{api_url.rstrip('/')}/v1/protocols\",\n        json={\"title\": title, \"document_text\": text},\n        timeout=30.0,\n    )\n    response.raise_for_status()\n    payload = cast(dict[str, str], response.json())\n    protocol_id = payload[\"protocol_id\"]\n\n    if auto_extract:\n        extract_resp = httpx.post(\n            f\"{api_url.rstrip('/')}/v1/protocols/{protocol_id}/extract\",\n            timeout=30.0,\n        )\n        extract_resp.raise_for_status()\n\n    return protocol_id\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.loader.bulk_load_protocols","title":"bulk_load_protocols","text":"<pre><code>bulk_load_protocols(manifest_path: Path = DEFAULT_MANIFEST_PATH, api_url: str = 'http://localhost:8000', limit: int = 50, auto_extract: bool = False) -&gt; list[str]\n</code></pre> <p>Bulk load protocols from a manifest into the database.</p> <p>Parameters:</p> Name Type Description Default <code>manifest_path</code> <code>Path</code> <p>Manifest JSONL containing downloaded PDFs.</p> <code>DEFAULT_MANIFEST_PATH</code> <code>api_url</code> <code>str</code> <p>API base URL.</p> <code>'http://localhost:8000'</code> <code>limit</code> <code>int</code> <p>Max number of records to load.</p> <code>50</code> <code>auto_extract</code> <code>bool</code> <p>Trigger extraction for each protocol after creation.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of created protocol IDs.</p> Source code in <code>components/data-pipeline/src/data_pipeline/loader.py</code> <pre><code>def bulk_load_protocols(\n    manifest_path: Path = DEFAULT_MANIFEST_PATH,\n    api_url: str = \"http://localhost:8000\",\n    limit: int = 50,\n    auto_extract: bool = False,\n) -&gt; list[str]:\n    \"\"\"Bulk load protocols from a manifest into the database.\n\n    Args:\n        manifest_path: Manifest JSONL containing downloaded PDFs.\n        api_url: API base URL.\n        limit: Max number of records to load.\n        auto_extract: Trigger extraction for each protocol after creation.\n\n    Returns:\n        List of created protocol IDs.\n    \"\"\"\n    records = ingest_local_protocols(manifest_path, limit=limit)\n    protocol_ids: list[str] = []\n\n    for record in records:\n        response = httpx.post(\n            f\"{api_url.rstrip('/')}/v1/protocols\",\n            json=_record_payload(record),\n            timeout=30.0,\n        )\n        if response.status_code != 200:\n            logger.warning(\n                \"Failed to create protocol %s (%s)\",\n                record.title,\n                response.text,\n            )\n            continue\n        payload = cast(dict[str, str], response.json())\n        protocol_id = payload[\"protocol_id\"]\n        protocol_ids.append(protocol_id)\n\n        if auto_extract:\n            extract_resp = httpx.post(\n                f\"{api_url.rstrip('/')}/v1/protocols/{protocol_id}/extract\",\n                timeout=30.0,\n            )\n            if extract_resp.status_code != 200:\n                logger.warning(\n                    \"Failed to extract criteria for %s (%s)\",\n                    protocol_id,\n                    extract_resp.text,\n                )\n\n    return protocol_ids\n</code></pre>"},{"location":"data-pipeline/docs/api/#data_pipeline.loader.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>CLI entrypoint.</p> Source code in <code>components/data-pipeline/src/data_pipeline/loader.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"CLI entrypoint.\"\"\"\n    parser = _build_parser()\n    args = parser.parse_args()\n\n    if args.pdf:\n        protocol_id = load_single_protocol(\n            args.pdf,\n            args.api_url,\n            auto_extract=not args.no_extract,\n        )\n        print(f\"Loaded protocol: {protocol_id}\")\n        return\n\n    protocol_ids = bulk_load_protocols(\n        manifest_path=args.manifest,\n        api_url=args.api_url,\n        limit=args.limit,\n        auto_extract=not args.no_extract,\n    )\n    print(f\"Loaded {len(protocol_ids)} protocols\")\n</code></pre>"},{"location":"diagrams/grounding-flow/","title":"Grounding Flow","text":"<pre><code>flowchart LR\n  criterion[CriterionText] --&gt; umls[UMLSSearch]\n  umls --&gt; candidates[SNOMEDCandidates]\n  candidates --&gt; rank[ModelRanking]\n  rank --&gt; review[HITLReview]\n  criterion --&gt; fieldmap[FieldRelationValueMapping]\n  fieldmap --&gt; review\n</code></pre>"},{"location":"diagrams/hitl-flow/","title":"HITL Flow","text":"<pre><code>flowchart LR\n  protocol[ProtocolUpload] --&gt; extract[CriteriaExtraction]\n  extract --&gt; ground[UMLSGrounding + FieldMapping]\n  ground --&gt; review[HITLReview]\n  review --&gt; feedback[FeedbackLog]\n  feedback --&gt; retrain[LoRAUpdate]\n</code></pre>"},{"location":"evaluation/","title":"evaluation","text":"<p>Metrics and reporting for extraction, grounding, and field/relation/value mapping quality.</p>"},{"location":"evaluation/#responsibilities","title":"Responsibilities","text":"<ul> <li>Compute extraction F1, SNOMED Top-1 accuracy, and field/relation/value mapping quality.</li> <li>Aggregate HITL acceptance metrics.</li> </ul>"},{"location":"evaluation/#key-module","title":"Key Module","text":"<ul> <li><code>evaluation/metrics.py</code></li> </ul>"},{"location":"evaluation/#example-usage","title":"Example Usage","text":"<pre><code>from evaluation.metrics import extraction_f1, snomed_top1_accuracy\n\nf1 = extraction_f1([\"age &gt;= 18\"], [\"age &gt;= 18\"])\ntop1 = snomed_top1_accuracy([\"372244006\"], [\"372244006\"])\n</code></pre>"},{"location":"evaluation/#tests","title":"Tests","text":"<pre><code>make check-all\n</code></pre>"},{"location":"evaluation/docs/api/","title":"evaluation API Reference","text":"<p>This page contains automatically generated API documentation for the evaluation component.</p>"},{"location":"evaluation/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"evaluation/docs/api/#evaluation","title":"evaluation","text":"<p>evaluation package.</p>"},{"location":"evaluation/docs/api/#evaluation-modules","title":"Modules","text":""},{"location":"evaluation/docs/api/#evaluation.metrics","title":"metrics","text":"<p>Evaluation metrics stubs.</p>"},{"location":"evaluation/docs/api/#evaluation.metrics-functions","title":"Functions","text":""},{"location":"evaluation/docs/api/#evaluation.metrics.extraction_f1","title":"extraction_f1","text":"<pre><code>extraction_f1(predicted: List[str], gold: List[str]) -&gt; float\n</code></pre> <p>Compute extraction F1 score for criteria lists.</p> <p>Normalizes strings before comparison to handle minor variations like trailing punctuation or case differences.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>List[str]</code> <p>Extracted criterion strings.</p> required <code>gold</code> <code>List[str]</code> <p>Gold-standard criterion strings.</p> required <p>Returns:</p> Type Description <code>float</code> <p>F1 score in the range [0.0, 1.0].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the inputs are empty or not comparable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extraction_f1([\"Age &gt;= 18\"], [\"Age &gt;= 18.\"])\n1.0\n&gt;&gt;&gt; extraction_f1([\"age &gt;= 18\"], [\"Age &gt;= 18\"])\n1.0\n</code></pre> Source code in <code>components/evaluation/src/evaluation/metrics.py</code> <pre><code>def extraction_f1(predicted: List[str], gold: List[str]) -&gt; float:\n    \"\"\"Compute extraction F1 score for criteria lists.\n\n    Normalizes strings before comparison to handle minor variations\n    like trailing punctuation or case differences.\n\n    Args:\n        predicted: Extracted criterion strings.\n        gold: Gold-standard criterion strings.\n\n    Returns:\n        F1 score in the range [0.0, 1.0].\n\n    Raises:\n        ValueError: If the inputs are empty or not comparable.\n\n    Examples:\n        &gt;&gt;&gt; extraction_f1([\"Age &gt;= 18\"], [\"Age &gt;= 18.\"])\n        1.0\n        &gt;&gt;&gt; extraction_f1([\"age &gt;= 18\"], [\"Age &gt;= 18\"])\n        1.0\n    \"\"\"\n    if not predicted or not gold:\n        raise ValueError(\"predicted and gold must be non-empty\")\n\n    # Normalize both predicted and gold before comparison\n    predicted_normalized = {_normalize_criterion_text(p) for p in predicted}\n    gold_normalized = {_normalize_criterion_text(g) for g in gold}\n\n    true_positives = len(predicted_normalized &amp; gold_normalized)\n    if true_positives == 0:\n        return 0.0\n\n    precision = true_positives / len(predicted_normalized)\n    recall = true_positives / len(gold_normalized)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n</code></pre>"},{"location":"evaluation/docs/api/#evaluation.metrics.snomed_top1_accuracy","title":"snomed_top1_accuracy","text":"<pre><code>snomed_top1_accuracy(predicted: List[str], gold: List[str]) -&gt; float\n</code></pre> <p>Compute Top-1 accuracy for SNOMED grounding.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>List[str]</code> <p>Predicted SNOMED codes.</p> required <code>gold</code> <code>List[str]</code> <p>Gold-standard SNOMED codes.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Top-1 accuracy in the range [0.0, 1.0].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the inputs are empty or not comparable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; snomed_top1_accuracy([\"372244006\"], [\"372244006\"])\n0.0\n</code></pre> Notes <p>This stub will be replaced by a metric implementation aligned with UMLS candidate ranking outputs.</p> Source code in <code>components/evaluation/src/evaluation/metrics.py</code> <pre><code>def snomed_top1_accuracy(predicted: List[str], gold: List[str]) -&gt; float:\n    \"\"\"Compute Top-1 accuracy for SNOMED grounding.\n\n    Args:\n        predicted: Predicted SNOMED codes.\n        gold: Gold-standard SNOMED codes.\n\n    Returns:\n        Top-1 accuracy in the range [0.0, 1.0].\n\n    Raises:\n        ValueError: If the inputs are empty or not comparable.\n\n    Examples:\n        &gt;&gt;&gt; snomed_top1_accuracy([\"372244006\"], [\"372244006\"])\n        0.0\n\n    Notes:\n        This stub will be replaced by a metric implementation aligned with\n        UMLS candidate ranking outputs.\n    \"\"\"\n    if not predicted or not gold:\n        raise ValueError(\"predicted and gold must be non-empty\")\n\n    comparisons = zip(predicted, gold)\n    matches = sum(\n        1\n        for predicted_code, gold_code in comparisons\n        if predicted_code == gold_code\n    )\n    return matches / len(gold)\n</code></pre>"},{"location":"evaluation/docs/api/#evaluation.metrics.field_mapping_accuracy","title":"field_mapping_accuracy","text":"<pre><code>field_mapping_accuracy(predicted: List[str], gold: List[str]) -&gt; float\n</code></pre> <p>Compute accuracy for field/relation/value mappings.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>List[str]</code> <p>Predicted normalized mapping strings (field|relation|value).</p> required <code>gold</code> <code>List[str]</code> <p>Gold-standard normalized mapping strings.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Accuracy in the range [0.0, 1.0].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the inputs are empty or not comparable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; field_mapping_accuracy([\"demographics.age|&gt;|75\"], [\"demographics.age|&gt;|75\"])\n0.0\n</code></pre> Notes <p>This stub will be replaced by a metric implementation aligned with normalized field/value parsing in the grounding service.</p> Source code in <code>components/evaluation/src/evaluation/metrics.py</code> <pre><code>def field_mapping_accuracy(predicted: List[str], gold: List[str]) -&gt; float:\n    \"\"\"Compute accuracy for field/relation/value mappings.\n\n    Args:\n        predicted: Predicted normalized mapping strings (field|relation|value).\n        gold: Gold-standard normalized mapping strings.\n\n    Returns:\n        Accuracy in the range [0.0, 1.0].\n\n    Raises:\n        ValueError: If the inputs are empty or not comparable.\n\n    Examples:\n        &gt;&gt;&gt; field_mapping_accuracy([\"demographics.age|&gt;|75\"], [\"demographics.age|&gt;|75\"])\n        0.0\n\n    Notes:\n        This stub will be replaced by a metric implementation aligned with\n        normalized field/value parsing in the grounding service.\n    \"\"\"\n    if not predicted or not gold:\n        raise ValueError(\"predicted and gold must be non-empty\")\n\n    comparisons = zip(predicted, gold)\n    matches = sum(\n        1\n        for predicted_value, gold_value in comparisons\n        if predicted_value == gold_value\n    )\n    return matches / len(gold)\n</code></pre>"},{"location":"evaluation/docs/api/#evaluation.metrics.hitl_acceptance_rate","title":"hitl_acceptance_rate","text":"<pre><code>hitl_acceptance_rate(actions: Iterable[str]) -&gt; float\n</code></pre> <p>Compute acceptance rate from HITL action labels.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>Iterable[str]</code> <p>Iterable of action labels (e.g., \"accept\", \"reject\").</p> required <p>Returns:</p> Type Description <code>float</code> <p>Acceptance rate as a float in the range [0.0, 1.0].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no actions are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hitl_acceptance_rate([\"accept\", \"reject\", \"accept\"])\n0.0\n</code></pre> Notes <p>This stub represents the acceptance metric tracked in the hackathon.</p> Source code in <code>components/evaluation/src/evaluation/metrics.py</code> <pre><code>def hitl_acceptance_rate(actions: Iterable[str]) -&gt; float:\n    \"\"\"Compute acceptance rate from HITL action labels.\n\n    Args:\n        actions: Iterable of action labels (e.g., \"accept\", \"reject\").\n\n    Returns:\n        Acceptance rate as a float in the range [0.0, 1.0].\n\n    Raises:\n        ValueError: If no actions are provided.\n\n    Examples:\n        &gt;&gt;&gt; hitl_acceptance_rate([\"accept\", \"reject\", \"accept\"])\n        0.0\n\n    Notes:\n        This stub represents the acceptance metric tracked in the hackathon.\n    \"\"\"\n    action_list = list(actions)\n    if not action_list:\n        raise ValueError(\"actions must be non-empty\")\n\n    accepted = sum(1 for action in action_list if action == \"accept\")\n    return accepted / len(action_list)\n</code></pre>"},{"location":"extraction-service/","title":"extraction-service","text":"<p>Wireframe extraction pipeline for MedGemma Task A. Produces atomic inclusion/exclusion criteria from protocol text.</p>"},{"location":"extraction-service/#responsibilities","title":"Responsibilities","text":"<ul> <li>Segment protocol text into candidate criteria.</li> <li>Classify criteria as inclusion/exclusion.</li> <li>Return evidence snippets and confidence scores.</li> </ul>"},{"location":"extraction-service/#key-module","title":"Key Module","text":"<ul> <li><code>extraction_service/pipeline.py</code></li> </ul>"},{"location":"extraction-service/#example-usage","title":"Example Usage","text":"<pre><code>from extraction_service.pipeline import extract_criteria\n\ncriteria = extract_criteria(\"Inclusion: Age &gt;= 18...\")\nfor item in criteria:\n    print(item.text, item.criterion_type, item.confidence)\n</code></pre>"},{"location":"extraction-service/#tests","title":"Tests","text":"<pre><code>make check-all\n</code></pre>"},{"location":"extraction-service/#notes","title":"Notes","text":"<p>This component is a stub. It documents the intended API for wiring MedGemma LoRA adapters.</p>"},{"location":"extraction-service/docs/api/","title":"extraction-service API Reference","text":"<p>This page contains automatically generated API documentation for the extraction-service component.</p>"},{"location":"extraction-service/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"extraction-service/docs/api/#extraction_service","title":"extraction_service","text":"<p>extraction-service package.</p>"},{"location":"extraction-service/docs/api/#extraction_service-modules","title":"Modules","text":""},{"location":"extraction-service/docs/api/#extraction_service.pipeline","title":"pipeline","text":"<p>Extraction pipeline for MedGemma Task A.</p>"},{"location":"extraction-service/docs/api/#extraction_service.pipeline-functions","title":"Functions","text":""},{"location":"extraction-service/docs/api/#extraction_service.pipeline.extract_criteria","title":"extract_criteria","text":"<pre><code>extract_criteria(document_text: str) -&gt; list[Criterion]\n</code></pre> <p>Extract atomic inclusion/exclusion criteria from protocol text.</p> <p>Parameters:</p> Name Type Description Default <code>document_text</code> <code>str</code> <p>Raw protocol text or extracted PDF text.</p> required <p>Returns:</p> Type Description <code>list[Criterion]</code> <p>A list of extracted criteria with type and confidence scores.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the document text is empty or not parseable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_criteria(\"Inclusion: Age &gt;= 18 years.\")\n[]\n</code></pre> Notes <p>This baseline implementation uses section detection and rule-based classification for prototyping.</p> Source code in <code>components/extraction-service/src/extraction_service/pipeline.py</code> <pre><code>def extract_criteria(document_text: str) -&gt; list[Criterion]:\n    \"\"\"Extract atomic inclusion/exclusion criteria from protocol text.\n\n    Args:\n        document_text: Raw protocol text or extracted PDF text.\n\n    Returns:\n        A list of extracted criteria with type and confidence scores.\n\n    Raises:\n        ValueError: If the document text is empty or not parseable.\n\n    Examples:\n        &gt;&gt;&gt; extract_criteria(\"Inclusion: Age &gt;= 18 years.\")\n        []\n\n    Notes:\n        This baseline implementation uses section detection and rule-based\n        classification for prototyping.\n    \"\"\"\n    if not document_text.strip():\n        raise ValueError(\"document_text is required\")\n\n    sections = detect_sections(document_text)\n    criteria: list[Criterion] = []\n\n    for section_type, section_text in sections.items():\n        sentences = split_into_candidate_sentences(section_text)\n        for sentence in sentences:\n            criterion_type = classify_criterion_type(sentence, section=section_type)\n            confidence = 0.9 if section_type != \"unknown\" else 0.7\n            criteria.append(\n                Criterion(\n                    id=\"\",\n                    text=sentence,\n                    criterion_type=criterion_type,\n                    confidence=confidence,\n                    snomed_codes=[],\n                    evidence_spans=[],\n                )\n            )\n\n    return criteria\n</code></pre>"},{"location":"extraction-service/docs/api/#extraction_service.pipeline.split_into_candidate_sentences","title":"split_into_candidate_sentences","text":"<pre><code>split_into_candidate_sentences(text: str) -&gt; list[str]\n</code></pre> <p>Split text into candidate criterion sentences.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Section text to split.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of candidate sentences.</p> Source code in <code>components/extraction-service/src/extraction_service/pipeline.py</code> <pre><code>def split_into_candidate_sentences(text: str) -&gt; list[str]:\n    \"\"\"Split text into candidate criterion sentences.\n\n    Args:\n        text: Section text to split.\n\n    Returns:\n        List of candidate sentences.\n    \"\"\"\n    if not text.strip():\n        return []\n\n    if \"\\n\" not in text and (\n        INLINE_INCLUSION.search(text) or INLINE_EXCLUSION.search(text)\n    ):\n        return _split_inline_criteria(text)\n\n    lines = text.split(\"\\n\")\n    candidates: list[str] = []\n    for line in lines:\n        cleaned = _normalize_candidate(BULLET_PATTERN.sub(\"\", line))\n        if not cleaned:\n            continue\n        if INCLUSION_HEADER.match(cleaned) or EXCLUSION_HEADER.match(cleaned):\n            continue\n        candidates.append(cleaned)\n    return candidates\n</code></pre>"},{"location":"extraction-service/docs/api/#extraction_service.pipeline.classify_criterion_type","title":"classify_criterion_type","text":"<pre><code>classify_criterion_type(candidate_text: str, section: str = 'unknown') -&gt; str\n</code></pre> <p>Classify criterion as inclusion or exclusion.</p> <p>Parameters:</p> Name Type Description Default <code>candidate_text</code> <code>str</code> <p>Criterion text.</p> required <code>section</code> <code>str</code> <p>Section context (\"inclusion\", \"exclusion\", or \"unknown\").</p> <code>'unknown'</code> <p>Returns:</p> Type Description <code>str</code> <p>Either \"inclusion\" or \"exclusion\".</p> Source code in <code>components/extraction-service/src/extraction_service/pipeline.py</code> <pre><code>def classify_criterion_type(candidate_text: str, section: str = \"unknown\") -&gt; str:\n    \"\"\"Classify criterion as inclusion or exclusion.\n\n    Args:\n        candidate_text: Criterion text.\n        section: Section context (\"inclusion\", \"exclusion\", or \"unknown\").\n\n    Returns:\n        Either \"inclusion\" or \"exclusion\".\n    \"\"\"\n    if not candidate_text.strip():\n        raise ValueError(\"candidate_text is required\")\n\n    lowered = candidate_text.lower()\n    for keyword in EXCLUSION_KEYWORDS:\n        if keyword in lowered:\n            return \"exclusion\"\n\n    if section == \"exclusion\":\n        return \"exclusion\"\n\n    return \"inclusion\"\n</code></pre>"},{"location":"extraction-service/docs/api/#extraction_service.pipeline.detect_sections","title":"detect_sections","text":"<pre><code>detect_sections(document_text: str) -&gt; Dict[str, str]\n</code></pre> <p>Detect inclusion/exclusion sections in protocol text.</p> <p>Parameters:</p> Name Type Description Default <code>document_text</code> <code>str</code> <p>Raw protocol text.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict mapping section type to section content.</p> Source code in <code>components/extraction-service/src/extraction_service/pipeline.py</code> <pre><code>def detect_sections(document_text: str) -&gt; Dict[str, str]:\n    \"\"\"Detect inclusion/exclusion sections in protocol text.\n\n    Args:\n        document_text: Raw protocol text.\n\n    Returns:\n        Dict mapping section type to section content.\n    \"\"\"\n    sections: Dict[str, str] = {}\n\n    inc_match = INCLUSION_HEADER.search(document_text)\n    exc_match = EXCLUSION_HEADER.search(document_text)\n\n    if not inc_match and not exc_match:\n        inc_match = INLINE_INCLUSION.search(document_text)\n        exc_match = INLINE_EXCLUSION.search(document_text)\n\n    if inc_match and exc_match:\n        if inc_match.start() &lt; exc_match.start():\n            sections[\"inclusion\"] = document_text[inc_match.end() : exc_match.start()]\n            sections[\"exclusion\"] = document_text[exc_match.end() :]\n        else:\n            sections[\"exclusion\"] = document_text[exc_match.end() : inc_match.start()]\n            sections[\"inclusion\"] = document_text[inc_match.end() :]\n    elif inc_match:\n        sections[\"inclusion\"] = document_text[inc_match.end() :]\n    elif exc_match:\n        sections[\"exclusion\"] = document_text[exc_match.end() :]\n\n    return sections\n</code></pre>"},{"location":"grounding-service/","title":"grounding-service","text":"<p>UMLS REST client and grounding helpers for Task B. Retrieves SNOMED candidates and field/relation/value mappings for each criterion.</p>"},{"location":"grounding-service/#responsibilities","title":"Responsibilities","text":"<ul> <li>Query UMLS for SNOMED candidates.</li> <li>Propose field/relation/value mappings for screening.</li> <li>Return ranked candidates with display names and confidence scores.</li> </ul>"},{"location":"grounding-service/#key-module","title":"Key Module","text":"<ul> <li><code>grounding_service/umls_client.py</code></li> </ul>"},{"location":"grounding-service/#example-usage","title":"Example Usage","text":"<pre><code>from grounding_service.umls_client import UmlsClient\n\nclient = UmlsClient(api_key=\"your-umls-api-key\")\nresults = client.search_snomed(\"stage III melanoma\")\nprint(results)\n</code></pre>"},{"location":"grounding-service/#tests","title":"Tests","text":"<pre><code>make check-all\n</code></pre>"},{"location":"grounding-service/#configuration","title":"Configuration","text":"<ul> <li><code>UMLS_API_KEY</code> (required)</li> <li><code>UMLS_BASE_URL</code> (default: <code>https://uts-ws.nlm.nih.gov/rest</code>)</li> <li><code>UMLS_TIMEOUT_SECONDS</code></li> </ul>"},{"location":"grounding-service/docs/api/","title":"grounding-service API Reference","text":"<p>This page contains automatically generated API documentation for the grounding-service component.</p>"},{"location":"grounding-service/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"grounding-service/docs/api/#grounding_service","title":"grounding_service","text":"<p>grounding-service package.</p>"},{"location":"grounding-service/docs/api/#grounding_service-modules","title":"Modules","text":""},{"location":"grounding-service/docs/api/#grounding_service.umls_client","title":"umls_client","text":"<p>UMLS SNOMED client with disk cache and retry.</p> <p>Environment variables: - UMLS_API_KEY: API key for UMLS (required). - UMLS_CACHE_DIR: Directory for disk cache (optional; defaults to   .cache/umls next to module). - UMLS_CACHE_TTL_SECONDS: TTL in seconds for cache entries (optional;   defaults to 7 days; must be &gt;0).</p>"},{"location":"grounding-service/docs/api/#grounding_service.umls_client-classes","title":"Classes","text":""},{"location":"grounding-service/docs/api/#grounding_service.umls_client.SnomedCandidate","title":"SnomedCandidate  <code>dataclass</code>","text":"<pre><code>SnomedCandidate(code: str, display: str, ontology: str, confidence: float)\n</code></pre> <p>SNOMED candidate returned from UMLS.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>SNOMED concept code.</p> required <code>display</code> <code>str</code> <p>Human-readable concept name.</p> required <code>ontology</code> <code>str</code> <p>Ontology label (e.g., SNOMEDCT_US).</p> required <code>confidence</code> <code>float</code> <p>Confidence or relevance score.</p> required"},{"location":"grounding-service/docs/api/#grounding_service.umls_client.FieldMappingSuggestion","title":"FieldMappingSuggestion  <code>dataclass</code>","text":"<pre><code>FieldMappingSuggestion(field: str, relation: str, value: str, confidence: float)\n</code></pre> <p>Field/relation/value mapping suggestion for a criterion.</p>"},{"location":"grounding-service/docs/api/#grounding_service.umls_client.UmlsClient","title":"UmlsClient","text":"<pre><code>UmlsClient(base_url: str | None = None, api_key: str | None = None, timeout: float | None = None)\n</code></pre> <p>Client for UMLS REST search endpoints with caching.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str | None</code> <p>Base URL for the UMLS REST API.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>UMLS API key (required).</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>HTTP timeout in seconds.</p> <code>None</code> <p>Initialize the UMLS client configuration.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>def __init__(\n    self,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    timeout: float | None = None,\n) -&gt; None:\n    \"\"\"Initialize the UMLS client configuration.\"\"\"\n    self.base_url: str = base_url or os.getenv(\"UMLS_BASE_URL\") or UMLS_DEFAULT_URL\n    self.api_key = api_key or os.getenv(\"UMLS_API_KEY\")\n    env_timeout = os.getenv(\"UMLS_TIMEOUT_SECONDS\")\n    self.timeout = (\n        float(env_timeout)\n        if env_timeout\n        else (timeout if timeout is not None else 10.0)\n    )\n    if not self.api_key:\n        raise ValueError(\"UMLS_API_KEY is required\")\n    self._http = httpx.Client(timeout=self.timeout)\n    cache_dir = os.getenv(\"UMLS_CACHE_DIR\")\n    default_cache = Path(user_cache_dir(\"grounding-service\", \"gemma\")) / \"umls\"\n    cache_path = Path(cache_dir) if cache_dir else default_cache\n    cache_path.mkdir(parents=True, exist_ok=True)\n    self._cache_dir = str(cache_path)\n    self._cache_ttl = self._parse_cache_ttl(os.getenv(\"UMLS_CACHE_TTL_SECONDS\"))\n    self._cache = diskcache.Cache(self._cache_dir)\n</code></pre> Functions search_snomed <pre><code>search_snomed(query: str, limit: int = 5) -&gt; list[SnomedCandidate]\n</code></pre> <p>Search SNOMED concepts via UMLS.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Free-text clinical concept to search.</p> required <code>limit</code> <code>int</code> <p>Maximum number of candidates to return.</p> <code>5</code> <p>Returns:</p> Type Description <code>list[SnomedCandidate]</code> <p>A list of candidate SNOMED concepts.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the query is empty or API key is missing.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>def search_snomed(self, query: str, limit: int = 5) -&gt; list[SnomedCandidate]:\n    \"\"\"Search SNOMED concepts via UMLS.\n\n    Args:\n        query: Free-text clinical concept to search.\n        limit: Maximum number of candidates to return.\n\n    Returns:\n        A list of candidate SNOMED concepts.\n\n    Raises:\n        ValueError: If the query is empty or API key is missing.\n    \"\"\"\n    if not query.strip():\n        raise ValueError(\"query is required\")\n\n    cache_key = f\"snomed:{query.lower()}:{limit}\"\n    cached = cast(list[SnomedCandidate] | None, self._cache.get(cache_key))\n    if cached is not None:\n        return cached\n\n    candidates = self._fetch_from_api(query, limit)\n    if self._cache_ttl:\n        self._cache.set(cache_key, candidates, expire=self._cache_ttl)\n    return candidates\n</code></pre> close <pre><code>close() -&gt; None\n</code></pre> <p>Close the HTTP client and release resources.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the HTTP client and release resources.\"\"\"\n    self._http.close()\n    try:\n        self._cache.close()\n    except Exception:\n        pass\n</code></pre> clear_cache <pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear the search cache.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the search cache.\"\"\"\n    self._cache.clear()\n</code></pre>"},{"location":"grounding-service/docs/api/#grounding_service.umls_client-functions","title":"Functions","text":""},{"location":"grounding-service/docs/api/#grounding_service.umls_client.umls_client_context","title":"umls_client_context","text":"<pre><code>umls_client_context(base_url: str | None = None, api_key: str | None = None, timeout: float | None = None) -&gt; Iterator['UmlsClient']\n</code></pre> <p>Provide a managed UmlsClient for safe usage elsewhere.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>@contextmanager\ndef umls_client_context(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    timeout: float | None = None,\n) -&gt; Iterator[\"UmlsClient\"]:\n    \"\"\"Provide a managed UmlsClient for safe usage elsewhere.\"\"\"\n    client = UmlsClient(base_url=base_url, api_key=api_key, timeout=timeout)\n    try:\n        yield client\n    finally:\n        client.close()\n</code></pre>"},{"location":"grounding-service/docs/api/#grounding_service.umls_client.propose_field_mapping","title":"propose_field_mapping","text":"<pre><code>propose_field_mapping(criterion_text: str) -&gt; list[FieldMappingSuggestion]\n</code></pre> <p>Propose field/relation/value mappings for a criterion.</p> <p>Parameters:</p> Name Type Description Default <code>criterion_text</code> <code>str</code> <p>Criterion text span to map.</p> required <p>Returns:</p> Type Description <code>list[FieldMappingSuggestion]</code> <p>A list of field mapping suggestions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the criterion text is empty.</p> Source code in <code>components/grounding-service/src/grounding_service/umls_client.py</code> <pre><code>def propose_field_mapping(criterion_text: str) -&gt; list[FieldMappingSuggestion]:\n    \"\"\"Propose field/relation/value mappings for a criterion.\n\n    Args:\n        criterion_text: Criterion text span to map.\n\n    Returns:\n        A list of field mapping suggestions.\n\n    Raises:\n        ValueError: If the criterion text is empty.\n    \"\"\"\n    if not criterion_text.strip():\n        raise ValueError(\"criterion_text is required\")\n\n    suggestions: list[FieldMappingSuggestion] = []\n    range_fields_added: set[str] = set()\n\n    for pattern, field, groups in FIELD_PATTERNS:\n        match = pattern.search(criterion_text)\n        if not match:\n            continue\n\n        if field in {\"demographics.age\", \"performance.ecog\"} and len(groups) == 2:\n            if \"-\" in match.group(0):\n                if field in range_fields_added:\n                    continue\n                low, high = match.group(groups[0]), match.group(groups[1])\n                suggestions.append(FieldMappingSuggestion(field, \"&gt;=\", low, 0.85))\n                suggestions.append(FieldMappingSuggestion(field, \"&lt;=\", high, 0.85))\n                range_fields_added.add(field)\n                continue\n\n        if field == \"demographics.sex\":\n            value = match.group(groups[0]).lower()\n            suggestions.append(FieldMappingSuggestion(field, \"=\", value, 0.9))\n            continue\n\n        if field == \"conditions.pregnancy\":\n            suggestions.append(FieldMappingSuggestion(field, \"=\", \"true\", 0.85))\n            continue\n\n        if len(groups) == 2:\n            relation = match.group(groups[0]) or \"=\"\n            value = match.group(groups[1])\n            suggestions.append(FieldMappingSuggestion(field, relation, value, 0.87))\n\n    return suggestions\n</code></pre>"},{"location":"hitl-ui/","title":"HITL UI","text":"<p>React/Vite frontend for nurse review of extracted criteria and evidence.</p>"},{"location":"hitl-ui/#scope","title":"Scope","text":"<ul> <li>Display protocols, criteria, and evidence snippets.</li> <li>Provide SNOMED candidate review and edit workflows.</li> <li>Provide field/relation/value mapping review and edits.</li> <li>Capture nurse feedback for audit and retraining.</li> </ul>"},{"location":"hitl-ui/#development","title":"Development","text":"<pre><code>npm install\nnpm run dev\n</code></pre>"},{"location":"hitl-ui/#structure","title":"Structure","text":"<ul> <li><code>src/screens</code> - Top-level views.</li> <li><code>src/features</code> - Feature-specific modules.</li> <li><code>src/components</code> - Shared UI components.</li> <li><code>src/design-system</code> - Tokens, theme, and UI primitives.</li> </ul>"},{"location":"overview/architecture/","title":"Architecture","text":"<p>The system is structured as a component-based monorepo with distinct services for API, extraction, grounding, data ingestion, evaluation, and UI.</p> <pre><code>flowchart LR\n  protocolSource[\"ProtocolSource (CT.gov)\"] --&gt; dataPipeline\n  dataPipeline --&gt; apiService\n  apiService --&gt; extractionService\n  apiService --&gt; groundingService\n  groundingService --&gt; umlsApi\n  hitlUi --&gt; apiService\n  apiService --&gt; database\n</code></pre>"},{"location":"overview/architecture/#key-integration-points","title":"Key Integration Points","text":"<ul> <li><code>POST /v1/protocols</code> to register protocol metadata and text/PDF.</li> <li><code>POST /v1/protocols/{id}/extract</code> to trigger extraction.</li> <li><code>POST /v1/criteria/{id}/ground</code> to retrieve SNOMED candidates and field/relation/value mapping suggestions.</li> <li><code>POST /v1/hitl/feedback</code> to log nurse actions.</li> </ul>"},{"location":"overview/architecture/#component-responsibilities","title":"Component Responsibilities","text":"Component Responsibilities <code>api-service</code> Orchestration, persistence, request validation, HITL feedback. <code>extraction-service</code> Extract atomic criteria and classify inclusion/exclusion. <code>grounding-service</code> UMLS REST lookup for SNOMED candidates + field/relation/value mapping suggestions. <code>data-pipeline</code> Protocol ingestion and normalization. <code>evaluation</code> Metrics (extraction F1, SNOMED Top-1, field mapping quality, HITL stats). <code>shared</code> Shared schemas and types. <code>hitl-ui</code> Nurse-facing review, edit, and evidence display."},{"location":"overview/architecture/#storage","title":"Storage","text":"<ul> <li><code>protocols</code>: protocol metadata and trial IDs.</li> <li><code>documents</code>: protocol text and provenance.</li> <li><code>criteria</code>: atomic criteria with type and evidence spans.</li> <li><code>groundings</code>: SNOMED candidates, confidence scores, and field/relation/value mappings.</li> <li><code>hitl_edits</code>: nurse actions for audit and retraining.</li> </ul>"},{"location":"overview/getting-started/","title":"Getting Started","text":""},{"location":"overview/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12</li> <li>UV package manager</li> <li>Node.js 18+ (for HITL UI)</li> <li>Docker (optional, for local DB)</li> </ul>"},{"location":"overview/getting-started/#setup","title":"Setup","text":"<pre><code>uv sync\nmake docs-build\n</code></pre> <p>To serve docs locally:</p> <pre><code>make docs-serve\n</code></pre> <p>To run the API service (wireframe stub):</p> <pre><code>cd components/api-service\nuv run uvicorn api_service.main:app --reload\n</code></pre> <p>To run the data pipeline stub:</p> <pre><code>cd components/data-pipeline\nuv run python -m data_pipeline.download_protocols --manifest-path data/protocols/manifest.jsonl\n</code></pre> <p>To run the HITL UI:</p> <pre><code>cd components/hitl-ui\nnpm install\nnpm run dev\n</code></pre>"},{"location":"overview/getting-started/#scripts","title":"Scripts","text":"<p>The repository includes utility scripts in the <code>scripts/</code> directory for common tasks.</p>"},{"location":"overview/getting-started/#protocol-download-tool","title":"Protocol Download Tool","text":"<p>The protocol download tool (<code>scripts/download_protocol_sources.py</code>) downloads clinical trial protocol PDFs from multiple sources for use in the data pipeline.</p> <p>Basic Usage:</p> <pre><code># Download protocols from all sources (default settings)\npython scripts/download_protocol_sources.py\n</code></pre> <p>Advanced Usage:</p> <pre><code># Download from specific sources only\npython scripts/download_protocol_sources.py --sources dac jmir\n\n# Limit the number of downloads\npython scripts/download_protocol_sources.py --max-per-source 10 --max-total 30\n\n# Use a custom output directory\npython scripts/download_protocol_sources.py --output-dir data/my-protocols\n</code></pre> <p>Available Sources: - <code>dac</code>: Data Access Committee protocol registry - <code>clinicaltrials</code>: ClinicalTrials.gov study protocols - <code>bmjopen</code>: BMJ Open protocol articles - <code>jmir</code>: JMIR Research Protocols</p> <p>Output: - PDFs are saved to <code>data/protocols/</code> (or specified directory) organized by source - A <code>manifest.jsonl</code> file tracks all download attempts with metadata</p> <p>Command-Line Options: - <code>--output-dir PATH</code>: Directory for downloaded PDFs (default: <code>data/protocols</code>) - <code>--sources SOURCE [SOURCE ...]</code>: Sources to download from (default: all) - <code>--max-per-source N</code>: Maximum PDFs per source (default: 50) - <code>--max-total N</code>: Maximum PDFs overall (default: 200) - <code>--timeout SECONDS</code>: Network timeout (default: 30) - <code>--sitemap-limit N</code>: Sitemap files to scan per source (default: 2)</p>"},{"location":"overview/getting-started/#development-scripts","title":"Development Scripts","text":"<ul> <li><code>scripts/create_component.sh</code>: Creates a new component with proper structure, dependencies, and documentation setup</li> <li><code>scripts/generate_components_overview.py</code>: Auto-generates the components overview page from component metadata</li> <li><code>scripts/update_root_navigation.py</code>: Updates the root mkdocs.yml navigation with all discovered components</li> </ul>"},{"location":"overview/getting-started/#testing","title":"Testing","text":"<p>Each Python component includes a Makefile with linting, type-checking, and test targets:</p> <pre><code>cd components/api-service\nmake check-all\n</code></pre>"},{"location":"overview/hackathon-plan/","title":"MedGemma Hackathon \u2013 Concise Operational Plan","text":"<p>Goal: Deliver a hackathon\u2011ready MedGemma HITL demo that extracts atomic inclusion/exclusion criteria from trial protocols, grounds them to SNOMED via the UMLS API, and maps criteria to field/relation/value (e.g., <code>demographics.age &gt; 75</code>), with a clear ElixirTrials integration story.</p>"},{"location":"overview/hackathon-plan/#1-project-brief","title":"1. Project Brief","text":""},{"location":"overview/hackathon-plan/#11-problem-impact","title":"1.1 Problem &amp; Impact","text":"<ul> <li>Clinical trial protocols are unstructured; screening is slow and error\u2011prone.</li> <li>We build an AI\u2011assisted system that:</li> <li>Extracts atomic inclusion/exclusion criteria from protocols.</li> <li>Maps them to SNOMED (via the UMLS REST API).</li> <li>Maps criteria to field + relation + value for EMR screening (e.g., <code>demographics.age &gt; 75</code>).</li> <li>Lets a nurse reviewer correct mappings through a simple HITL UI.</li> <li>Target time savings: ~65\u201370% nurse time per protocol vs. manual review.  </li> </ul>"},{"location":"overview/hackathon-plan/#12-success-criteria-hackathon","title":"1.2 Success Criteria (Hackathon)","text":"<p>Judging\u2011aligned: 1. Human\u2011Centered AI    - Simple, transparent HITL UI.    - Shows provenance (evidence snippets) + confidence scores.    - Nurse can accept/edit SNOMED codes and field/relation/value mappings inline.</p> <ol> <li>Technical Rigor</li> <li>MedGemma 1.5\u20134B\u2011IT with LoRA adapters for:<ul> <li>Task A: Criteria extraction.</li> <li>Task B: SNOMED grounding + field/relation/value mapping.</li> </ul> </li> <li> <p>8\u2011bit quantized inference on DGX Spark\u2011class hardware.</p> </li> <li> <p>Impact &amp; Feasibility</p> </li> <li>Measured extraction F1 and SNOMED Top\u20111 accuracy.</li> <li>Measured time per protocol: baseline vs AI\u2011assisted.</li> <li>Clear story to plug into ElixirTrials/Cauldron.</li> </ol> <p>Quantitative Targets (Hackathon): - Extraction F1: \u2265 0.85 - SNOMED Top\u20111 accuracy: \u2265 0.80 - Nurse acceptance rate: \u2265 70% - Time per protocol: \u2265 60% reduction vs manual.</p>"},{"location":"overview/hackathon-plan/#2-scope","title":"2. Scope","text":""},{"location":"overview/hackathon-plan/#21-in-scope-hackathon","title":"2.1 In Scope (Hackathon)","text":"<ul> <li>Ingest public ClinicalTrials.gov protocols (200\u2013300).</li> <li>Extract atomic inclusion/exclusion criteria.</li> <li>Ground criteria to SNOMED via the UMLS REST API (no external dependencies beyond public APIs).</li> <li>Map criteria to field + relation + value for EMR screening (field\u2011value mapping).</li> <li>HITL backend:</li> <li>Save criteria, suggested codes, and nurse edits.</li> <li>Simple review UI (Gradio or minimal React view).</li> <li>LoRA fine\u2011tuning of MedGemma on nurse\u2011validated labels.</li> <li>8\u2011bit quantized inference on DGX Spark\u2011class workstation.</li> <li>Kaggle deliverables:</li> <li>Writeup.</li> <li>Training/eval notebook.</li> <li>GitHub repo.</li> <li>3\u20135 min video demo.</li> </ul>"},{"location":"overview/hackathon-plan/#22-out-of-scope-posthackathon-only","title":"2.2 Out of Scope (Post\u2011Hackathon Only)","text":"<ul> <li>Federated learning / multi\u2011site aggregation.</li> <li>EMR / FHIR / Redox integration.</li> <li>Patient\u2011level screening &amp; matching.</li> <li>Full Triplane/Screening integration.</li> </ul>"},{"location":"overview/hackathon-plan/#3-architecture-highlevel","title":"3. Architecture (High\u2011Level)","text":""},{"location":"overview/hackathon-plan/#31-data-flow","title":"3.1 Data Flow","text":"<ol> <li>Protocol Upload</li> <li>Input: PDF/text from ClinicalTrials.gov.</li> <li> <p>Stored as <code>protocol</code> and <code>document</code> rows.</p> </li> <li> <p>Extraction</p> </li> <li>Endpoint: <code>POST /v1/protocols/{id}/extract</code></li> <li> <p>Uses MedGemma (LoRA Task A) \u2192 atomic criteria + type.</p> </li> <li> <p>Grounding</p> </li> <li>Endpoint: <code>POST /v1/criteria/{id}/ground</code></li> <li> <p>UMLS REST API (terminology lookup) + MedGemma (LoRA Task B) \u2192 ranked SNOMED codes + field/relation/value mapping.</p> </li> <li> <p>HITL Review</p> </li> <li>Nurse UI fetches:<ul> <li>Criteria list.</li> <li>Suggested SNOMED codes + confidence + evidence snippets.</li> <li>Suggested field/relation/value mapping (e.g., <code>demographics.age &gt; 75</code>).</li> </ul> </li> <li> <p>Nurse edits logged in <code>hitl_edits</code>.</p> </li> <li> <p>Retraining Loop (Single\u2011site)</p> </li> <li>Export edits as new training data.</li> <li>Periodic LoRA re\u2011training of MedGemma adapters.</li> </ol>"},{"location":"overview/hackathon-plan/#32-minimal-api-contract-hackathon","title":"3.2 Minimal API Contract (Hackathon)","text":"<p>Base URL: <code>/v1</code></p> <ul> <li><code>POST /v1/protocols</code></li> <li>Create protocol metadata + initial document (text or PDF).</li> <li><code>POST /v1/protocols/{protocolId}/extract</code></li> <li>Trigger criteria extraction (sync is enough).</li> <li><code>GET /v1/protocols/{protocolId}/criteria</code></li> <li>List criteria for review.</li> <li><code>PATCH /v1/criteria/{criterionId}</code></li> <li>Edit criterion text/type/etc.</li> <li><code>POST /v1/criteria/{criterionId}/ground</code></li> <li>Get SNOMED candidates via the UMLS REST API + field/relation/value mapping.</li> <li><code>POST /v1/hitl/feedback</code></li> <li>Log nurse actions (accept/remove/add code etc).</li> </ul> <p>(Full OpenAPI spec lives in <code>docs/api_spec.yaml</code>.)</p>"},{"location":"overview/hackathon-plan/#33-core-components","title":"3.3 Core Components","text":"Component Tech / Notes Backend API FastAPI (Python) Model Inference MedGemma 1.5\u20134B\u2011IT + LoRA (8\u2011bit) Terminology UMLS REST API (no custom wrapper) DB PostgreSQL (protocols, criteria, edits) HITL UI Gradio or minimal Cauldron\u2011style view Hardware DGX Spark\u2011class GPU workstation"},{"location":"overview/hackathon-plan/#34-umls-integration","title":"3.4 UMLS Integration","text":"<p>Why UMLS: - Official NLM terminology services with SNOMED coverage. - Simple REST API with API key authentication. - Public/research access with license acceptance. - Multi\u2011terminology support; we restrict to SNOMEDCT_US for grounding.</p> <p>Usage Pattern:</p> <pre><code># Lookup term in UMLS\nGET https://uts-ws.nlm.nih.gov/rest/search/current?string=stage%20III%20melanoma&amp;sabs=SNOMEDCT_US&amp;returnIdType=code&amp;pageSize=5&amp;apiKey=YOUR_API_KEY\n\n# Response includes SNOMED codes + preferred names\n{\n  \"result\": {\n    \"results\": [\n      {\n        \"ui\": \"372244006\",\n        \"name\": \"Malignant melanoma, stage III\",\n        \"rootSource\": \"SNOMEDCT_US\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Implementation: - Simple HTTP client in <code>grounding_service/umls_client.py</code>. - Required <code>UMLS_API_KEY</code> (env var). - Optional in\u2011memory caching for frequent lookups.</p>"},{"location":"overview/hackathon-plan/#4-sprints-daily-execution","title":"4. Sprints &amp; Daily Execution","text":""},{"location":"overview/hackathon-plan/#41-week-1-data-umls-integration-jan-1521","title":"4.1 Week 1 \u2013 Data &amp; UMLS Integration (Jan 15\u201321)","text":"<p>Goals: - Have protocols in DB. - UMLS API client working + cached. - Minimal extraction pipeline running (base model).</p> <p>Tasks:</p> <ul> <li>Data &amp; DB</li> <li>Implement <code>scripts/download_protocols.py</code> to fetch 200\u2013300 oncology/cardio trials from ClinicalTrials.gov.</li> <li>Store <code>{nct_id, title, i/e text, condition, phase}</code> in PostgreSQL.</li> <li> <p>Define DB schema:</p> <ul> <li><code>protocols</code>, <code>documents</code>, <code>criteria</code>, <code>groundings</code>, <code>hitl_edits</code>.</li> </ul> </li> <li> <p>UMLS Integration</p> </li> <li>Implement <code>grounding_service/umls_client.py</code>:<ul> <li>HTTP client to UMLS REST API.</li> <li>Query term \u2192 get SNOMEDCT_US candidates.</li> <li>Simple in\u2011memory cache (TTL configurable).</li> </ul> </li> <li> <p>CLI smoke test: terms like \"stage III melanoma\", \"ECOG PS 0\u20131\".</p> </li> <li> <p>Baseline Extraction</p> </li> <li>Implement rough parsing of I/E sections into candidate criteria (regex/sentence splitting).</li> <li>Run base MedGemma (no LoRA) for:<ul> <li>Type classification (inclusion/exclusion).</li> <li>Draft SNOMED suggestions via UMLS.</li> <li>Draft field/relation/value mapping suggestions.</li> </ul> </li> </ul> <p>Decision / Risk: - By end of Week 1:   - If UMLS API rate limits hit \u2192 reduce pageSize and rely on caching.</p>"},{"location":"overview/hackathon-plan/#42-week-2-hitl-labeling-jan-2228","title":"4.2 Week 2 \u2013 HITL &amp; Labeling (Jan 22\u201328)","text":"<p>Goals: - Usable HITL UI for nurse. - ~1,000 labeled examples for training.</p> <p>Tasks:</p> <ul> <li>HITL UI (MVP)</li> <li>Build simple UI (Gradio or minimal React) with:<ul> <li>Protocol text on left, highlighted criterion.</li> <li>Criterion card on right:</li> <li>Type, text, confidence.</li> <li>SNOMED candidates with checkboxes.</li> <li>Field/relation/value mapping preview + edits.</li> <li>Add/remove code, rationale.</li> </ul> </li> <li> <p>Wire to backend:</p> <ul> <li><code>GET /protocols/{id}/criteria</code></li> <li><code>POST /criteria/{id}/ground</code></li> <li><code>POST /hitl/feedback</code>.</li> </ul> </li> <li> <p>Annotation Workflow</p> </li> <li>Pre\u2011label criteria using base MedGemma + UMLS:<ul> <li>Save as <code>criteria_prelabeled.jsonl</code>.</li> </ul> </li> <li>Define gold label schema (criterion text, type, SNOMED codes, field/relation/value, evidence spans).</li> <li>Target: ~1,000 validated criteria (spanning ~120 protocols).</li> <li> <p>Coordinate nurse schedule (~200h total across hackathon).</p> </li> <li> <p>Dataset Preparation</p> </li> <li>Create train/val/test splits:<ul> <li>Train: 1,000 examples (~120 protocols).</li> <li>Val: 100 examples (~15 protocols).</li> <li>Test: 100 examples (held\u2011out conditions).</li> </ul> </li> </ul> <p>Decision / Risk: - If by end of Week 2:   - &lt;700 labeled examples \u2192 reduce model ambition (e.g., only Task B LoRA) and focus on demo quality.</p>"},{"location":"overview/hackathon-plan/#43-week-3-training-backend-jan-29feb-4","title":"4.3 Week 3 \u2013 Training &amp; Backend (Jan 29\u2013Feb 4)","text":"<p>Goals: - LoRA adapters trained and deployed. - Backend API stable. - Basic metrics computed.</p> <p>Tasks:</p> <ul> <li>Model Training</li> <li>Task A (Extraction):<ul> <li>LoRA config: <code>r=16</code>, <code>alpha=32</code>, target <code>q_proj/v_proj/o_proj</code>.</li> <li>3 epochs, batch size 4 (grad accum 4), lr <code>2e\u20114</code>.</li> </ul> </li> <li>Task B (Grounding):<ul> <li>Separate LoRA adapter, same config.</li> <li>Training data from nurse\u2011validated mapping + UMLS candidates.</li> </ul> </li> <li> <p>Use 8\u2011bit QLoRA on DGX Spark; save adapters in <code>models/</code>.</p> </li> <li> <p>Backend Hardening</p> </li> <li>Implement final versions of:<ul> <li><code>POST /protocols</code></li> <li><code>POST /protocols/{id}/extract</code></li> <li><code>GET /protocols/{id}/criteria</code></li> <li><code>PATCH /criteria/{id}</code></li> <li><code>POST /criteria/{id}/ground</code></li> <li><code>POST /hitl/feedback</code></li> </ul> </li> <li>Add logging &amp; basic error handling.</li> <li> <p>Add minimal tests (Pytest) for core API paths.</p> </li> <li> <p>Evaluation</p> </li> <li>Run on test set:<ul> <li>Extraction F1.</li> <li>SNOMED Top\u20111 accuracy.</li> <li>Field/relation/value mapping quality.</li> <li>Nurse acceptance rate on small test batch.</li> </ul> </li> <li>Measure:<ul> <li>Time per protocol (manual vs AI\u2011assisted with small pilot).</li> <li>Inference latency per protocol (&lt;15s target).</li> </ul> </li> </ul> <p>Stretch (Only if above done early): - Toy federated simulation with 2\u20133 synthetic \"sites\" using site splits.</p>"},{"location":"overview/hackathon-plan/#44-week-4-demo-writeup-polish-feb-512","title":"4.4 Week 4 \u2013 Demo, Writeup &amp; Polish (Feb 5\u201312)","text":"<p>Goals: - Kaggle submission ready. - Demo smooth and reproducible.</p> <p>Tasks:</p> <ul> <li>Demo Experience</li> <li>End\u2011to\u2011end flow:<ol> <li>Upload or select existing protocol.</li> <li>Run extraction.</li> <li>Show criteria list with SNOMED codes + field/relation/value &amp; confidence.</li> <li>Perform a couple of nurse edits.</li> <li>Show metrics/time\u2011saved panel.</li> </ol> </li> <li> <p>Ensure no manual dev hacks (one\u2011command run via Docker Compose).</p> </li> <li> <p>Kaggle Writeup</p> </li> <li> <p>Sections (with word limits):</p> <ol> <li>Problem &amp; Impact (~300 words).</li> <li>Architecture (~400\u2013500 words + 1 diagram).</li> <li>Human\u2011in\u2011the\u2011loop workflow (~300 words).</li> <li>Model &amp; Training (~400\u2013500 words).</li> <li>Results &amp; Error Analysis (~400\u2013500 words).</li> <li>ElixirTrials Integration Roadmap (~300\u2013400 words).</li> <li>Reproducibility &amp; Open Source (~200 words).</li> </ol> </li> <li> <p>Notebook</p> </li> <li> <p><code>notebooks/training_pipeline.ipynb</code>:</p> <ul> <li>Load sample data.</li> <li>Show preprocessing.</li> <li>Demo LoRA training steps (small subset for Kaggle runtime).</li> <li>Show metrics + inference example.</li> </ul> </li> <li> <p>GitHub Repo</p> </li> <li>Structure:     <code>text     gemma-hackathon/       backend/         main.py         models/         umls_client.py         tests/       notebooks/       data/ (small sample only)       docs/         api_spec.yaml         architecture.md         umls_notes.md       docker-compose.yml       README.md</code></li> <li> <p>README:</p> <ul> <li>One\u2011command run.</li> <li>Hardware assumptions.</li> <li>UMLS API licensing notes.</li> <li>\"Demo only \u2013 not for clinical use\" disclaimer.</li> </ul> </li> <li> <p>Video (3\u20135 min)</p> </li> <li>Script:<ul> <li>30s: Problem framing.</li> <li>60\u201390s: Live demo of upload \u2192 extraction \u2192 review.</li> <li>60s: Metrics summary.</li> <li>30\u201360s: ElixirTrials integration story.</li> </ul> </li> </ul>"},{"location":"overview/hackathon-plan/#5-key-implementation-notes","title":"5. Key Implementation Notes","text":""},{"location":"overview/hackathon-plan/#51-umls-approval-already-secured","title":"5.1 UMLS Approval Already Secured","text":"<p>\u2705 No NLM license delay; proceed directly with full deployment.</p>"},{"location":"overview/hackathon-plan/#52-umls-as-single-source-of-truth","title":"5.2 UMLS as Single Source of Truth","text":"<ul> <li>Use the UMLS REST API for all terminology lookups (SNOMED, LOINC, RxNorm, ICD\u201110).</li> <li>No custom wrapper needed; simple HTTP client + optional caching.</li> </ul>"},{"location":"overview/hackathon-plan/#53-data-privacy","title":"5.3 Data Privacy","text":"<ul> <li>All protocols and edits stay in\u2011prem during hackathon.</li> <li>Only LoRA adapter weights and anonymized metrics shared post\u2011hackathon.</li> </ul>"},{"location":"overview/hackathon-plan/#6-metrics-decision-gates","title":"6. Metrics &amp; Decision Gates","text":""},{"location":"overview/hackathon-plan/#61-core-metrics","title":"6.1 Core Metrics","text":"<ul> <li>Extraction (Task A)</li> <li>F1 on test set.</li> <li>Grounding (Task B)</li> <li>SNOMED Top\u20111 accuracy.</li> <li>Field Mapping</li> <li>Field/relation/value mapping quality.</li> <li>HITL</li> <li>Nurse acceptance rate.</li> <li>Time per protocol (manual vs AI\u2011assisted).</li> </ul>"},{"location":"overview/hackathon-plan/#62-go-nogo-for-stretch-work-federated-prototype","title":"6.2 Go / No\u2011Go for Stretch Work (Federated Prototype)","text":"<ul> <li>Only attempt if by end of Week 3:</li> <li>Extraction F1 \u2265 0.85.</li> <li>SNOMED Top\u20111 \u2265 0.80.</li> <li>End\u2011to\u2011end demo stable.</li> <li>At least 120 protocols annotated.</li> </ul> <p>Otherwise, invest Week 4 fully in polish, metrics clarity, and submission quality.</p>"},{"location":"overview/hackathon-plan/#7-submission-checklist","title":"7. Submission Checklist","text":""},{"location":"overview/hackathon-plan/#t72-hours","title":"T\u201172 Hours","text":"<ul> <li>[ ] Run Docker Compose from clean environment.</li> <li>[ ] Run Kaggle notebook from top to bottom on Kaggle GPU.</li> <li>[ ] Freeze model artifacts (tagged release in GitHub).</li> </ul>"},{"location":"overview/hackathon-plan/#t24-hours","title":"T\u201124 Hours","text":"<ul> <li>[ ] Finalize Kaggle writeup text.</li> <li>[ ] Upload demo video (YouTube unlisted).</li> <li>[ ] Verify all GitHub + notebook links.</li> </ul>"},{"location":"overview/hackathon-plan/#t0","title":"T\u20110","text":"<ul> <li>[ ] Submit Kaggle entry and confirm appearance in challenge.</li> <li>[ ] Backup repo + models to long\u2011term storage.</li> <li>[ ] Note learnings + next\u2011steps for ElixirTrials production track.</li> </ul>"},{"location":"overview/project/","title":"Project Overview","text":"<p>The MedGemma hackathon demo focuses on protocol ingestion, criteria extraction, UMLS grounding, field/relation/value mapping, and HITL review. The objective is to reduce nurse review time while preserving transparency and provenance.</p>"},{"location":"overview/project/#scope","title":"Scope","text":"<ul> <li>Protocol ingestion and parsing.</li> <li>Criteria extraction and classification.</li> <li>SNOMED grounding via the UMLS API.</li> <li>Field/relation/value mapping for EMR screening.</li> <li>HITL UI for review and corrections.</li> <li>Minimal evaluation metrics.</li> </ul>"},{"location":"overview/project/#success-criteria","title":"Success Criteria","text":"<ul> <li>Extraction F1 &gt;= 0.85.</li> <li>SNOMED Top-1 accuracy &gt;= 0.80.</li> <li>Field/relation/value mapping quality tracked (target TBD).</li> <li>Nurse acceptance rate &gt;= 70%.</li> <li>Time per protocol reduction &gt;= 60% vs manual.</li> </ul>"},{"location":"overview/project/#deliverables","title":"Deliverables","text":"<ul> <li>End-to-end demo (protocol -&gt; extraction -&gt; grounding + field mapping -&gt; HITL edits).</li> <li>API spec and component documentation.</li> <li>Training and evaluation notebook.</li> <li>3-5 minute demo video.</li> </ul>"},{"location":"overview/project/#out-of-scope-post-hackathon","title":"Out of Scope (Post-Hackathon)","text":"<ul> <li>EMR/FHIR integration.</li> <li>Multi-site federated learning.</li> <li>Patient-level screening and matching.</li> </ul>"},{"location":"phases/week-1/","title":"Week 1 - Data &amp; UMLS Integration","text":"<ul> <li>Ingest protocols into the database.</li> <li>Implement UMLS client with caching.</li> <li>Baseline extraction pipeline.</li> </ul> <p>See <code>docs/overview/hackathon-plan.md</code> for full details.</p>"},{"location":"phases/week-2/","title":"Week 2 - HITL &amp; Labeling","text":"<ul> <li>Build HITL UI MVP.</li> <li>Collect labeled examples.</li> <li>Prepare dataset splits.</li> </ul> <p>See <code>docs/overview/hackathon-plan.md</code> for full details.</p>"},{"location":"phases/week-3/","title":"Week 3 - Training &amp; Backend","text":"<ul> <li>Train LoRA adapters.</li> <li>Harden API endpoints.</li> <li>Add basic evaluation metrics.</li> </ul> <p>See <code>docs/overview/hackathon-plan.md</code> for full details.</p>"},{"location":"phases/week-4/","title":"Week 4 - Demo &amp; Polish","text":"<ul> <li>End-to-end demo flow.</li> <li>Finalize writeup and video.</li> <li>Ensure reproducibility.</li> </ul> <p>See <code>docs/overview/hackathon-plan.md</code> for full details.</p>"},{"location":"shared/","title":"shared","text":"<p>Shared data models and schemas used by API services and the UI.</p>"},{"location":"shared/#responsibilities","title":"Responsibilities","text":"<ul> <li>Define canonical data structures for protocols, criteria, groundings, and field/relation/value mappings.</li> <li>Keep type definitions synchronized across services.</li> </ul>"},{"location":"shared/#key-module","title":"Key Module","text":"<ul> <li><code>shared/models.py</code></li> </ul>"},{"location":"shared/#example-usage","title":"Example Usage","text":"<pre><code>from shared.models import Criterion\n\ncriterion = Criterion(\n    id=\"crit-1\",\n    text=\"Age &gt;= 18 years\",\n    criterion_type=\"inclusion\",\n    confidence=0.92,\n    snomed_codes=[\"371273006\"],\n)\n</code></pre>"},{"location":"shared/#tests","title":"Tests","text":"<pre><code>make check-all\n</code></pre>"},{"location":"shared/docs/api/","title":"shared API Reference","text":"<p>This page contains automatically generated API documentation for the shared component.</p>"},{"location":"shared/docs/api/#api-documentation","title":"API Documentation","text":""},{"location":"shared/docs/api/#shared","title":"shared","text":"<p>shared package.</p>"},{"location":"shared/docs/api/#shared-modules","title":"Modules","text":""},{"location":"shared/docs/api/#shared.models","title":"models","text":"<p>Shared data models for API and UI.</p>"},{"location":"shared/docs/api/#shared.models-classes","title":"Classes","text":""},{"location":"shared/docs/api/#shared.models.Criterion","title":"Criterion  <code>dataclass</code>","text":"<pre><code>Criterion(id: str, text: str, criterion_type: str, confidence: float, snomed_codes: List[str], evidence_spans: List[EvidenceSpan] = list())\n</code></pre> <p>Atomic criterion extracted from a protocol.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Stable identifier for the criterion.</p> required <code>text</code> <code>str</code> <p>Criterion text.</p> required <code>criterion_type</code> <code>str</code> <p>Inclusion or exclusion label.</p> required <code>confidence</code> <code>float</code> <p>Model confidence score.</p> required <code>snomed_codes</code> <code>List[str]</code> <p>SNOMED codes attached to the criterion.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; Criterion(\n...     id=\"crit-1\",\n...     text=\"Age &gt;= 18 years\",\n...     criterion_type=\"inclusion\",\n...     confidence=0.92,\n...     snomed_codes=[\"371273006\"],\n... )\nCriterion(\n...     id='crit-1',\n...     text='Age &gt;= 18 years',\n...     criterion_type='inclusion',\n...     confidence=0.92,\n...     snomed_codes=['371273006'],\n... )\n</code></pre> Notes <p>Evidence spans and grounding candidates are stored separately.</p>"},{"location":"shared/docs/api/#shared.models.EvidenceSpan","title":"EvidenceSpan  <code>dataclass</code>","text":"<pre><code>EvidenceSpan(start_char: int, end_char: int, source_doc_id: str)\n</code></pre> <p>Evidence span linking criterion to source document.</p> <p>Parameters:</p> Name Type Description Default <code>start_char</code> <code>int</code> <p>Starting character offset in source.</p> required <code>end_char</code> <code>int</code> <p>Ending character offset in source.</p> required <code>source_doc_id</code> <code>str</code> <p>Document identifier for provenance.</p> required"},{"location":"shared/docs/api/#shared.models.FieldMapping","title":"FieldMapping  <code>dataclass</code>","text":"<pre><code>FieldMapping(field: str, relation: str, value: str, confidence: Optional[float] = None)\n</code></pre> <p>Field/relation/value mapping attached to a criterion.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Target field path (e.g., demographics.age).</p> required <code>relation</code> <code>str</code> <p>Comparison operator (e.g., &gt;, &gt;=, =).</p> required <code>value</code> <code>str</code> <p>Normalized value string (e.g., 75).</p> required <code>confidence</code> <code>Optional[float]</code> <p>Optional confidence score.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; FieldMapping(\n...     field=\"demographics.age\",\n...     relation=\"&gt;\",\n...     value=\"75\",\n...     confidence=0.87,\n... )\nFieldMapping(\n...     field='demographics.age',\n...     relation='&gt;',\n...     value='75',\n...     confidence=0.87,\n... )\n</code></pre> Functions to_string <pre><code>to_string() -&gt; str\n</code></pre> <p>Serialize to JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation of the field mapping.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fm = FieldMapping(field=\"demographics.age\", relation=\"&gt;=\", value=\"18\")\n&gt;&gt;&gt; fm.to_string()\n'{\"field\":\"demographics.age\",\"relation\":\"&gt;=\",\"value\":\"18\"}'\n</code></pre> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Serialize to JSON string.\n\n    Returns:\n        JSON string representation of the field mapping.\n\n    Examples:\n        &gt;&gt;&gt; fm = FieldMapping(field=\"demographics.age\", relation=\"&gt;=\", value=\"18\")\n        &gt;&gt;&gt; fm.to_string()\n        '{\"field\":\"demographics.age\",\"relation\":\"&gt;=\",\"value\":\"18\"}'\n    \"\"\"\n    data = {\n        \"field\": self.field,\n        \"relation\": self.relation,\n        \"value\": self.value,\n    }\n    return json.dumps(data, separators=(\",\", \":\"))\n</code></pre> from_string <code>classmethod</code> <pre><code>from_string(value: str) -&gt; FieldMapping\n</code></pre> <p>Deserialize from JSON string or pipe-delimited string.</p> <p>Supports backward compatibility with pipe-delimited format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>JSON string or pipe-delimited string.</p> required <p>Returns:</p> Type Description <code>FieldMapping</code> <p>FieldMapping instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string format is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; json_str = '{\"field\":\"demographics.age\",\"relation\":\"&gt;=\",\"value\":\"18\"}'\n&gt;&gt;&gt; FieldMapping.from_string(json_str)\nFieldMapping(field='demographics.age', relation='&gt;=', value='18')\n&gt;&gt;&gt; FieldMapping.from_string(\"demographics.age|&gt;=|18\")  # Legacy\nFieldMapping(field='demographics.age', relation='&gt;=', value='18')\n</code></pre> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"FieldMapping\":\n    \"\"\"Deserialize from JSON string or pipe-delimited string.\n\n    Supports backward compatibility with pipe-delimited format.\n\n    Args:\n        value: JSON string or pipe-delimited string.\n\n    Returns:\n        FieldMapping instance.\n\n    Raises:\n        ValueError: If the string format is invalid.\n\n    Examples:\n        &gt;&gt;&gt; json_str = '{\"field\":\"demographics.age\",\"relation\":\"&gt;=\",\"value\":\"18\"}'\n        &gt;&gt;&gt; FieldMapping.from_string(json_str)\n        FieldMapping(field='demographics.age', relation='&gt;=', value='18')\n        &gt;&gt;&gt; FieldMapping.from_string(\"demographics.age|&gt;=|18\")  # Legacy\n        FieldMapping(field='demographics.age', relation='&gt;=', value='18')\n    \"\"\"\n    # Try JSON first (new format)\n    if value.strip().startswith(\"{\"):\n        try:\n            data = json.loads(value)\n            return cls(\n                field=data[\"field\"],\n                relation=data[\"relation\"],\n                value=data[\"value\"],\n            )\n        except (json.JSONDecodeError, KeyError) as e:\n            raise ValueError(\n                f\"Invalid field mapping JSON string: '{value}'. Error: {e}\"\n            ) from e\n\n    # Fall back to pipe-delimited format (backward compatibility)\n    parts = value.split(\"|\")\n    if len(parts) != 3:\n        raise ValueError(\n            \"Invalid field mapping string: \"\n            f\"'{value}'. Expected JSON format or pipe-delimited format \"\n            \"'field|relation|value' (e.g., 'demographics.age|&gt;=|18').\"\n        )\n    return cls(field=parts[0], relation=parts[1], value=parts[2])\n</code></pre>"},{"location":"shared/docs/api/#shared.models.Protocol","title":"Protocol  <code>dataclass</code>","text":"<pre><code>Protocol(id: str, title: str, nct_id: str, condition: str, phase: str, source: Optional[str] = None, registry_id: Optional[str] = None, registry_type: Optional[str] = None)\n</code></pre> <p>Protocol metadata tracked by the API.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Stable protocol identifier.</p> required <code>title</code> <code>str</code> <p>Human-readable trial title.</p> required <code>nct_id</code> <code>str</code> <p>ClinicalTrials.gov identifier.</p> required <code>condition</code> <code>str</code> <p>Primary disease/condition.</p> required <code>phase</code> <code>str</code> <p>Trial phase label.</p> required <code>source</code> <code>Optional[str]</code> <p>Optional source identifier (clinicaltrials, dac, etc.).</p> <code>None</code> <code>registry_id</code> <code>Optional[str]</code> <p>Optional registry-specific identifier.</p> <code>None</code> <code>registry_type</code> <code>Optional[str]</code> <p>Optional registry type label (nct, isrctn, ctis).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Protocol(\n...     id=\"proto-1\",\n...     title=\"Example Trial\",\n...     nct_id=\"NCT00000000\",\n...     condition=\"Melanoma\",\n...     phase=\"Phase 2\",\n...     source=\"clinicaltrials\",\n...     registry_id=\"NCT00000000\",\n...     registry_type=\"nct\",\n... )\nProtocol(\n...     id='proto-1',\n...     title='Example Trial',\n...     nct_id='NCT00000000',\n...     condition='Melanoma',\n...     phase='Phase 2',\n...     source='clinicaltrials',\n...     registry_id='NCT00000000',\n...     registry_type='nct',\n... )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.Document","title":"Document  <code>dataclass</code>","text":"<pre><code>Document(id: str, protocol_id: str, text: str, source_url: Optional[str])\n</code></pre> <p>Protocol document content and provenance.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Document identifier.</p> required <code>protocol_id</code> <code>str</code> <p>Associated protocol identifier.</p> required <code>text</code> <code>str</code> <p>Extracted protocol text.</p> required <code>source_url</code> <code>Optional[str]</code> <p>Optional source URL for provenance.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; Document(\n...     id=\"doc-1\",\n...     protocol_id=\"proto-1\",\n...     text=\"Inclusion: ...\",\n...     source_url=None,\n... )\nDocument(\n...     id='doc-1',\n...     protocol_id='proto-1',\n...     text='Inclusion: ...',\n...     source_url=None,\n... )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.GroundingCandidate","title":"GroundingCandidate  <code>dataclass</code>","text":"<pre><code>GroundingCandidate(code: str, display: str, confidence: float)\n</code></pre> <p>SNOMED candidate for a criterion grounding decision.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>SNOMED concept code.</p> required <code>display</code> <code>str</code> <p>Human-readable concept description.</p> required <code>confidence</code> <code>float</code> <p>Model or retrieval confidence score.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; GroundingCandidate(\n...     code=\"372244006\",\n...     display=\"Malignant melanoma, stage III\",\n...     confidence=0.92,\n... )\nGroundingCandidate(\n...     code='372244006',\n...     display='Malignant melanoma, stage III',\n...     confidence=0.92,\n... )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.HitlEdit","title":"HitlEdit  <code>dataclass</code>","text":"<pre><code>HitlEdit(id: str, criterion_id: str, action: str, note: Optional[str], snomed_code_added: Optional[str] = None, snomed_code_removed: Optional[str] = None, field_mapping_added: Optional[str] = None, field_mapping_removed: Optional[str] = None)\n</code></pre> <p>Human-in-the-loop edit captured from a nurse reviewer.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Edit identifier.</p> required <code>criterion_id</code> <code>str</code> <p>Criterion updated by the reviewer.</p> required <code>action</code> <code>str</code> <p>Action label (accept, reject, add, edit).</p> required <code>note</code> <code>Optional[str]</code> <p>Optional rationale or comment.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; HitlEdit(\n...     id=\"edit-1\",\n...     criterion_id=\"crit-1\",\n...     action=\"accept\",\n...     note=\"Matches protocol text\",\n... )\nHitlEdit(\n...     id='edit-1',\n...     criterion_id='crit-1',\n...     action='accept',\n...     note='Matches protocol text',\n... )\n</code></pre>"},{"location":"shared/docs/api/#shared.models-functions","title":"Functions","text":""},{"location":"shared/docs/api/#shared.models.build_criterion","title":"build_criterion","text":"<pre><code>build_criterion(*, id: str = 'crit-1', text: str = 'Age &gt;= 18 years', criterion_type: str = 'inclusion', confidence: float = 0.92, snomed_codes: Optional[List[str]] = None, evidence_spans: Optional[List[EvidenceSpan]] = None) -&gt; Criterion\n</code></pre> <p>Create a Criterion instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_criterion(\n    *,\n    id: str = \"crit-1\",\n    text: str = \"Age &gt;= 18 years\",\n    criterion_type: str = \"inclusion\",\n    confidence: float = 0.92,\n    snomed_codes: Optional[List[str]] = None,\n    evidence_spans: Optional[List[EvidenceSpan]] = None,\n) -&gt; Criterion:\n    \"\"\"Create a Criterion instance with defaults for tests and examples.\"\"\"\n    return Criterion(\n        id=id,\n        text=text,\n        criterion_type=criterion_type,\n        confidence=confidence,\n        snomed_codes=snomed_codes or [\"371273006\"],\n        evidence_spans=evidence_spans or [],\n    )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.build_field_mapping","title":"build_field_mapping","text":"<pre><code>build_field_mapping(*, field: str = 'demographics.age', relation: str = '&gt;=', value: str = '18', confidence: Optional[float] = 0.87) -&gt; FieldMapping\n</code></pre> <p>Create a FieldMapping instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_field_mapping(\n    *,\n    field: str = \"demographics.age\",\n    relation: str = \"&gt;=\",\n    value: str = \"18\",\n    confidence: Optional[float] = 0.87,\n) -&gt; FieldMapping:\n    \"\"\"Create a FieldMapping instance with defaults for tests and examples.\"\"\"\n    return FieldMapping(\n        field=field,\n        relation=relation,\n        value=value,\n        confidence=confidence,\n    )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.build_protocol","title":"build_protocol","text":"<pre><code>build_protocol(*, id: str = 'proto-1', title: str = 'Example Trial', nct_id: str = 'NCT00000000', condition: str = 'Melanoma', phase: str = 'Phase 2', source: Optional[str] = None, registry_id: Optional[str] = None, registry_type: Optional[str] = None) -&gt; Protocol\n</code></pre> <p>Create a Protocol instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_protocol(\n    *,\n    id: str = \"proto-1\",\n    title: str = \"Example Trial\",\n    nct_id: str = \"NCT00000000\",\n    condition: str = \"Melanoma\",\n    phase: str = \"Phase 2\",\n    source: Optional[str] = None,\n    registry_id: Optional[str] = None,\n    registry_type: Optional[str] = None,\n) -&gt; Protocol:\n    \"\"\"Create a Protocol instance with defaults for tests and examples.\"\"\"\n    return Protocol(\n        id=id,\n        title=title,\n        nct_id=nct_id,\n        condition=condition,\n        phase=phase,\n        source=source,\n        registry_id=registry_id,\n        registry_type=registry_type,\n    )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.build_document","title":"build_document","text":"<pre><code>build_document(*, id: str = 'doc-1', protocol_id: str = 'proto-1', text: str = 'Inclusion: Age &gt;= 18.', source_url: Optional[str] = None) -&gt; Document\n</code></pre> <p>Create a Document instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_document(\n    *,\n    id: str = \"doc-1\",\n    protocol_id: str = \"proto-1\",\n    text: str = \"Inclusion: Age &gt;= 18.\",\n    source_url: Optional[str] = None,\n) -&gt; Document:\n    \"\"\"Create a Document instance with defaults for tests and examples.\"\"\"\n    return Document(\n        id=id,\n        protocol_id=protocol_id,\n        text=text,\n        source_url=source_url,\n    )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.build_grounding_candidate","title":"build_grounding_candidate","text":"<pre><code>build_grounding_candidate(*, code: str = '372244006', display: str = 'Malignant melanoma, stage III', confidence: float = 0.92) -&gt; GroundingCandidate\n</code></pre> <p>Create a GroundingCandidate instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_grounding_candidate(\n    *,\n    code: str = \"372244006\",\n    display: str = \"Malignant melanoma, stage III\",\n    confidence: float = 0.92,\n) -&gt; GroundingCandidate:\n    \"\"\"Create a GroundingCandidate instance with defaults for tests and examples.\"\"\"\n    return GroundingCandidate(\n        code=code,\n        display=display,\n        confidence=confidence,\n    )\n</code></pre>"},{"location":"shared/docs/api/#shared.models.build_hitl_edit","title":"build_hitl_edit","text":"<pre><code>build_hitl_edit(*, id: str = 'edit-1', criterion_id: str = 'crit-1', action: str = 'accept', note: Optional[str] = 'Matches protocol text', snomed_code_added: Optional[str] = None, snomed_code_removed: Optional[str] = None, field_mapping_added: Optional[str] = None, field_mapping_removed: Optional[str] = None) -&gt; HitlEdit\n</code></pre> <p>Create a HitlEdit instance with defaults for tests and examples.</p> Source code in <code>components/shared/src/shared/models.py</code> <pre><code>def build_hitl_edit(\n    *,\n    id: str = \"edit-1\",\n    criterion_id: str = \"crit-1\",\n    action: str = \"accept\",\n    note: Optional[str] = \"Matches protocol text\",\n    snomed_code_added: Optional[str] = None,\n    snomed_code_removed: Optional[str] = None,\n    field_mapping_added: Optional[str] = None,\n    field_mapping_removed: Optional[str] = None,\n) -&gt; HitlEdit:\n    \"\"\"Create a HitlEdit instance with defaults for tests and examples.\"\"\"\n    return HitlEdit(\n        id=id,\n        criterion_id=criterion_id,\n        action=action,\n        note=note,\n        snomed_code_added=snomed_code_added,\n        snomed_code_removed=snomed_code_removed,\n        field_mapping_added=field_mapping_added,\n        field_mapping_removed=field_mapping_removed,\n    )\n</code></pre>"}]}