{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LoRA Training Pipeline\n",
                "\n",
                "This notebook demonstrates how to fine-tune MedGemma using the `model-training` component."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies and local package\n",
                "!pip install -q -e ../components/model-training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from model_training.dataset import load_training_data, format_extraction_prompt, format_grounding_prompt\n",
                "from model_training.config import get_lora_config, load_model_and_tokenizer\n",
                "from model_training.train import train_model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "Load training data from JSONL files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = \"../data/training/criteria_train.jsonl\"\n",
                "# Ensure data exists (or use dummy path for demo)\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    print(f\"Warning: {DATA_PATH} not found. Please export data first.\")\n",
                "else:\n",
                "    train_data = load_training_data(DATA_PATH)\n",
                "    print(f\"Loaded {len(train_data)} examples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Format Data\n",
                "Format data for the specific task (Extraction or Grounding)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Formatting for Extraction Task\n",
                "if 'train_data' in locals():\n",
                "    formatted_dataset = train_data.map(format_extraction_prompt)\n",
                "    print(formatted_dataset[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configure Model\n",
                "Load 4-bit/8-bit quantized model and apply LoRA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"google/medgemma-4b-it\" # Replace with accessible model if needed\n",
                "\n",
                "lora_config = get_lora_config(r=16, alpha=32, dropout=0.05)\n",
                "\n",
                "# Note: This requires GPU. Set load_in_8bit=False for CPU (but will be slow/OOM)\n",
                "try:\n",
                "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME, lora_config, load_in_8bit=True)\n",
                "    model.print_trainable_parameters()\n",
                "except Exception as e:\n",
                "    print(f\"Could not load model (expected if no GPU/internet): {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train\n",
                "Run the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT_DIR = \"../models/extraction_lora\"\n",
                "\n",
                "if 'model' in locals() and 'formatted_dataset' in locals():\n",
                "    train_model(\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        train_dataset=formatted_dataset,\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        num_epochs=3,\n",
                "        batch_size=4\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}